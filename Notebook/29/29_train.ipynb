{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc4f3190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import timm\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as torchdata\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from timm.models.layers import SelectAdaptivePool2d\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torchlibrosa.stft import LogmelFilterBank, Spectrogram\n",
    "from torchlibrosa.augmentation import SpecAugmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e05f64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "285849bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../')\n",
    "import src.utils as utils\n",
    "from config_ import CFG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea73aa95",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c59b66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_ROOT = Path(\"/home/knikaido/work/Cornell-Birdcall-Identification/data\")\n",
    "RAW_DATA = INPUT_ROOT / \"birdsong-recognition\"\n",
    "TRAIN_AUDIO_DIR = RAW_DATA / \"train_audio\"\n",
    "TRAIN_RESAMPLED_AUDIO_DIRS = [\n",
    "  INPUT_ROOT / \"birdsong-resampled-train-audio-{:0>2}\".format(i)  for i in range(5)\n",
    "]\n",
    "OUTPUT_DIR = Path('./output/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e299df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Data #\n",
    "######################\n",
    "# train_datadir = MAIN_DATA_DIR / 'train_short_audio'\n",
    "# train_csv = MAIN_DATA_DIR / 'train_metadata.csv'\n",
    "# train_soundscape = MAIN_DATA_DIR / 'train_soundscape_labels.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21723f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "if DEBUG:\n",
    "    CFG.epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fe580f",
   "metadata": {},
   "source": [
    "## My func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53f847fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveformDataset(torchdata.Dataset):\n",
    "    def __init__(self,\n",
    "                 df: pd.DataFrame,\n",
    "                 img_size=224,\n",
    "                 waveform_transforms=None,\n",
    "                 period=20,\n",
    "                 validation=False):\n",
    "        self.df = df\n",
    "        self.img_size = img_size\n",
    "        self.waveform_transforms = waveform_transforms\n",
    "        self.period = period\n",
    "        self.validation = validation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        sample = self.df.loc[idx, :]\n",
    "        wav_path = sample[\"file_path\"]\n",
    "        ebird_code = sample[\"ebird_code\"]\n",
    "\n",
    "        y, sr = sf.read(wav_path)\n",
    "\n",
    "        len_y = len(y)\n",
    "        effective_length = sr * self.period\n",
    "        if len_y < effective_length:\n",
    "            new_y = np.zeros(effective_length, dtype=y.dtype)\n",
    "            if not self.validation:\n",
    "                start = np.random.randint(effective_length - len_y)\n",
    "            else:\n",
    "                start = 0\n",
    "            new_y[start:start + len_y] = y\n",
    "            y = new_y.astype(np.float32)\n",
    "        elif len_y > effective_length:\n",
    "            if not self.validation:\n",
    "                start = np.random.randint(len_y - effective_length)\n",
    "            else:\n",
    "                start = 0\n",
    "            y = y[start:start + effective_length].astype(np.float32)\n",
    "        else:\n",
    "            y = y.astype(np.float32)\n",
    "\n",
    "        y = np.nan_to_num(y)\n",
    "\n",
    "        if self.waveform_transforms:\n",
    "            y = self.waveform_transforms(y)\n",
    "\n",
    "        y = np.nan_to_num(y)\n",
    "\n",
    "        labels = np.zeros(len(CFG.target_columns), dtype=float)\n",
    "        labels[CFG.target_columns.index(ebird_code)] = 1.0\n",
    "\n",
    "        return {\n",
    "            \"image\": y,\n",
    "            \"targets\": labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11cfeee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(phase: str):\n",
    "    transforms = CFG.transforms\n",
    "    if transforms is None:\n",
    "        return None\n",
    "    else:\n",
    "        if transforms[phase] is None:\n",
    "            return None\n",
    "        trns_list = []\n",
    "        for trns_conf in transforms[phase]:\n",
    "            trns_name = trns_conf[\"name\"]\n",
    "            trns_params = {} if trns_conf.get(\"params\") is None else \\\n",
    "                trns_conf[\"params\"]\n",
    "            if globals().get(trns_name) is not None:\n",
    "                trns_cls = globals()[trns_name]\n",
    "                trns_list.append(trns_cls(**trns_params))\n",
    "\n",
    "        if len(trns_list) > 0:\n",
    "            return Compose(trns_list)\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        \n",
    "class Normalize:\n",
    "    def __call__(self, y: np.ndarray):\n",
    "        max_vol = np.abs(y).max()\n",
    "        y_vol = y * 1 / max_vol\n",
    "        return np.asfortranarray(y_vol)\n",
    "\n",
    "\n",
    "class Compose:\n",
    "    def __init__(self, transforms: list):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, y: np.ndarray):\n",
    "        for trns in self.transforms:\n",
    "            y = trns(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cd15cd",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "590f25aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layer(layer):\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "    if hasattr(layer, \"bias\"):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "\n",
    "\n",
    "def init_bn(bn):\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.0)\n",
    "\n",
    "\n",
    "def init_weights(model):\n",
    "    classname = model.__class__.__name__\n",
    "    if classname.find(\"Conv2d\") != -1:\n",
    "        nn.init.xavier_uniform_(model.weight, gain=np.sqrt(2))\n",
    "        model.bias.data.fill_(0)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        model.weight.data.normal_(1.0, 0.02)\n",
    "        model.bias.data.fill_(0)\n",
    "    elif classname.find(\"GRU\") != -1:\n",
    "        for weight in model.parameters():\n",
    "            if len(weight.size()) > 1:\n",
    "                nn.init.orghogonal_(weight.data)\n",
    "    elif classname.find(\"Linear\") != -1:\n",
    "        model.weight.data.normal_(0, 0.01)\n",
    "        model.bias.data.zero_()\n",
    "\n",
    "\n",
    "def do_mixup(x: torch.Tensor, mixup_lambda: torch.Tensor):\n",
    "    \"\"\"Mixup x of even indexes (0, 2, 4, ...) with x of odd indexes\n",
    "    (1, 3, 5, ...).\n",
    "    Args:\n",
    "      x: (batch_size * 2, ...)\n",
    "      mixup_lambda: (batch_size * 2,)\n",
    "    Returns:\n",
    "      out: (batch_size, ...)\n",
    "    \"\"\"\n",
    "    out = (x[0::2].transpose(0, -1) * mixup_lambda[0::2] +\n",
    "           x[1::2].transpose(0, -1) * mixup_lambda[1::2]).transpose(0, -1)\n",
    "    return out\n",
    "\n",
    "\n",
    "class Mixup(object):\n",
    "    def __init__(self, mixup_alpha, random_seed=1234):\n",
    "        \"\"\"Mixup coefficient generator.\n",
    "        \"\"\"\n",
    "        self.mixup_alpha = mixup_alpha\n",
    "        self.random_state = np.random.RandomState(random_seed)\n",
    "\n",
    "    def get_lambda(self, batch_size):\n",
    "        \"\"\"Get mixup random coefficients.\n",
    "        Args:\n",
    "          batch_size: int\n",
    "        Returns:\n",
    "          mixup_lambdas: (batch_size,)\n",
    "        \"\"\"\n",
    "        mixup_lambdas = []\n",
    "        for n in range(0, batch_size, 2):\n",
    "            lam = self.random_state.beta(\n",
    "                self.mixup_alpha, self.mixup_alpha, 1)[0]\n",
    "            mixup_lambdas.append(lam)\n",
    "            mixup_lambdas.append(1. - lam)\n",
    "\n",
    "        return torch.from_numpy(np.array(mixup_lambdas, dtype=np.float32))\n",
    "\n",
    "\n",
    "def interpolate(x: torch.Tensor, ratio: int):\n",
    "    \"\"\"Interpolate data in time domain. This is used to compensate the\n",
    "    resolution reduction in downsampling of a CNN.\n",
    "    Args:\n",
    "      x: (batch_size, time_steps, classes_num)\n",
    "      ratio: int, ratio to interpolate\n",
    "    Returns:\n",
    "      upsampled: (batch_size, time_steps * ratio, classes_num)\n",
    "    \"\"\"\n",
    "    (batch_size, time_steps, classes_num) = x.shape\n",
    "    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n",
    "    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n",
    "    return upsampled\n",
    "\n",
    "\n",
    "def pad_framewise_output(framewise_output: torch.Tensor, frames_num: int):\n",
    "    \"\"\"Pad framewise_output to the same length as input frames. The pad value\n",
    "    is the same as the value of the last frame.\n",
    "    Args:\n",
    "      framewise_output: (batch_size, frames_num, classes_num)\n",
    "      frames_num: int, number of frames to pad\n",
    "    Outputs:\n",
    "      output: (batch_size, frames_num, classes_num)\n",
    "    \"\"\"\n",
    "    output = F.interpolate(\n",
    "        framewise_output.unsqueeze(1),\n",
    "        size=(frames_num, framewise_output.size(2)),\n",
    "        align_corners=True,\n",
    "        mode=\"bilinear\").squeeze(1)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def gem(x: torch.Tensor, p=3, eps=1e-6):\n",
    "    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1. / p)\n",
    "\n",
    "\n",
    "class GeM(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.p = nn.Parameter(torch.ones(1) * p)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return gem(x, p=self.p, eps=self.eps)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + f\"(p={self.p.data.tolist()[0]:.4f}, eps={self.eps})\"\n",
    "\n",
    "\n",
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return x\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d32455c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TimmSED(nn.Module):\n",
    "    def __init__(self, base_model_name: str, pretrained=False, num_classes=24, in_channels=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = Spectrogram(n_fft=CFG.n_fft, hop_length=CFG.hop_length,\n",
    "                                                 win_length=CFG.n_fft, window=\"hann\", center=True, pad_mode=\"reflect\",\n",
    "                                                 freeze_parameters=True)\n",
    "\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = LogmelFilterBank(sr=CFG.sample_rate, n_fft=CFG.n_fft,\n",
    "                                                 n_mels=CFG.n_mels, fmin=CFG.fmin, fmax=CFG.fmax, ref=1.0, amin=1e-10, top_db=None,\n",
    "                                                 freeze_parameters=True)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(CFG.n_mels)\n",
    "\n",
    "        base_model = timm.create_model(\n",
    "            base_model_name, pretrained=pretrained, in_chans=in_channels)\n",
    "        layers = list(base_model.children())[:-2]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "\n",
    "        if hasattr(base_model, \"fc\"):\n",
    "            in_features = base_model.fc.in_features\n",
    "        else:\n",
    "            in_features = base_model.classifier.in_features\n",
    "        self.fc1 = nn.Linear(in_features, in_features, bias=True)\n",
    "        self.att_block = AttBlockV2(\n",
    "            in_features, num_classes, activation=\"sigmoid\")\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_layer(self.fc1)\n",
    "        init_bn(self.bn0)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # (batch_size, 1, time_steps, freq_bins)\n",
    "        x = self.spectrogram_extractor(input)\n",
    "        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n",
    "        frames_num = x.shape[2]\n",
    "\n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "\n",
    "        x = x.transpose(2, 3)\n",
    "        # (batch_size, channels, freq, frames)\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # (batch_size, channels, frames)\n",
    "        x = torch.mean(x, dim=2)\n",
    "\n",
    "        # channel smoothing\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n",
    "        logit = torch.sum(norm_att * self.att_block.cla(x), dim=2)\n",
    "        segmentwise_logit = self.att_block.cla(x).transpose(1, 2)\n",
    "        segmentwise_output = segmentwise_output.transpose(1, 2)\n",
    "\n",
    "        interpolate_ratio = frames_num // segmentwise_output.size(1)\n",
    "\n",
    "        # Get framewise output\n",
    "        framewise_output = interpolate(segmentwise_output,\n",
    "                                       interpolate_ratio)\n",
    "        framewise_output = pad_framewise_output(framewise_output, frames_num)\n",
    "\n",
    "        framewise_logit = interpolate(segmentwise_logit, interpolate_ratio)\n",
    "        framewise_logit = pad_framewise_output(framewise_logit, frames_num)\n",
    "\n",
    "        output_dict = {\n",
    "            \"framewise_output\": framewise_output,\n",
    "            \"segmentwise_output\": segmentwise_output,\n",
    "            \"logit\": logit,\n",
    "            \"framewise_logit\": framewise_logit,\n",
    "            \"clipwise_output\": clipwise_output\n",
    "        }\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470fe857",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "085c0c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/rfcx-species-audio-detection/discussion/213075\n",
    "class BCEFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        bce_loss = nn.BCEWithLogitsLoss(reduction='none')(preds, targets)\n",
    "        probas = torch.sigmoid(preds)\n",
    "        loss = targets * self.alpha * \\\n",
    "            (1. - probas)**self.gamma * bce_loss + \\\n",
    "            (1. - targets) * probas**self.gamma * bce_loss\n",
    "        loss = loss.mean()\n",
    "        return loss\n",
    "\n",
    "\n",
    "class BCEFocal2WayLoss(nn.Module):\n",
    "    def __init__(self, weights=[1, 1], class_weights=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.focal = BCEFocalLoss()\n",
    "\n",
    "        self.weights = weights\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        input_ = input[\"logit\"]\n",
    "        target = target.float()\n",
    "\n",
    "        framewise_output = input[\"framewise_logit\"]\n",
    "        clipwise_output_with_max, _ = framewise_output.max(dim=1)\n",
    "\n",
    "        loss = self.focal(input_, target)\n",
    "        aux_loss = self.focal(clipwise_output_with_max, target)\n",
    "\n",
    "        return self.weights[0] * loss + self.weights[1] * aux_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed779c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "__CRITERIONS__ = {\n",
    "    \"BCEFocalLoss\": BCEFocalLoss,\n",
    "    \"BCEFocal2WayLoss\": BCEFocal2WayLoss\n",
    "}\n",
    "\n",
    "\n",
    "def get_criterion():\n",
    "    if hasattr(nn, CFG.loss_name):\n",
    "        return nn.__getattribute__(CFG.loss_name)(**CFG.loss_params)\n",
    "    elif __CRITERIONS__.get(CFG.loss_name) is not None:\n",
    "        return __CRITERIONS__[CFG.loss_name](**CFG.loss_params)\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c64377b",
   "metadata": {},
   "source": [
    "## Training Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c0ba447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom optimizer\n",
    "__OPTIMIZERS__ = {}\n",
    "\n",
    "\n",
    "def get_optimizer(model: nn.Module):\n",
    "    optimizer_name = CFG.optimizer_name\n",
    "    if optimizer_name == \"SAM\":\n",
    "        base_optimizer_name = CFG.base_optimizer\n",
    "        if __OPTIMIZERS__.get(base_optimizer_name) is not None:\n",
    "            base_optimizer = __OPTIMIZERS__[base_optimizer_name]\n",
    "        else:\n",
    "            base_optimizer = optim.__getattribute__(base_optimizer_name)\n",
    "        return SAM(model.parameters(), base_optimizer, **CFG.optimizer_params)\n",
    "\n",
    "    if __OPTIMIZERS__.get(optimizer_name) is not None:\n",
    "        return __OPTIMIZERS__[optimizer_name](model.parameters(),\n",
    "                                              **CFG.optimizer_params)\n",
    "    else:\n",
    "        return optim.__getattribute__(optimizer_name)(model.parameters(),\n",
    "                                                      **CFG.optimizer_params)\n",
    "\n",
    "\n",
    "def get_scheduler(optimizer):\n",
    "    scheduler_name = CFG.scheduler_name\n",
    "\n",
    "    if scheduler_name is None:\n",
    "        return\n",
    "    else:\n",
    "        return optim.lr_scheduler.__getattribute__(scheduler_name)(\n",
    "            optimizer, **CFG.scheduler_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011bb6b9",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd00e157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "utils.set_seed(CFG.seed)\n",
    "\n",
    "# validation\n",
    "splitter = getattr(model_selection, CFG.split)(**CFG.split_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "229e3c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_RESAMPLED_AUDIO_DIRS[0] / \"train_mod.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37e28b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21375, 38)\n",
      "(21375, 3)\n",
      "(21375, 39)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>playback_used</th>\n",
       "      <th>ebird_code</th>\n",
       "      <th>channels</th>\n",
       "      <th>date</th>\n",
       "      <th>pitch</th>\n",
       "      <th>duration</th>\n",
       "      <th>filename</th>\n",
       "      <th>speed</th>\n",
       "      <th>species</th>\n",
       "      <th>...</th>\n",
       "      <th>primary_label</th>\n",
       "      <th>longitude</th>\n",
       "      <th>length</th>\n",
       "      <th>time</th>\n",
       "      <th>recordist</th>\n",
       "      <th>license</th>\n",
       "      <th>resampled_sampling_rate</th>\n",
       "      <th>resampled_filename</th>\n",
       "      <th>resampled_channels</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.5</td>\n",
       "      <td>no</td>\n",
       "      <td>aldfly</td>\n",
       "      <td>1 (mono)</td>\n",
       "      <td>2013-05-25</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>25</td>\n",
       "      <td>XC134874.mp3</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>Alder Flycatcher</td>\n",
       "      <td>...</td>\n",
       "      <td>Empidonax alnorum_Alder Flycatcher</td>\n",
       "      <td>-92.962</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>8:00</td>\n",
       "      <td>Jonathon Jongsma</td>\n",
       "      <td>Creative Commons Attribution-ShareAlike 3.0</td>\n",
       "      <td>32000</td>\n",
       "      <td>XC134874.wav</td>\n",
       "      <td>1 (mono)</td>\n",
       "      <td>/home/knikaido/work/Cornell-Birdcall-Identific...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>no</td>\n",
       "      <td>aldfly</td>\n",
       "      <td>2 (stereo)</td>\n",
       "      <td>2013-05-27</td>\n",
       "      <td>both</td>\n",
       "      <td>36</td>\n",
       "      <td>XC135454.mp3</td>\n",
       "      <td>both</td>\n",
       "      <td>Alder Flycatcher</td>\n",
       "      <td>...</td>\n",
       "      <td>Empidonax alnorum_Alder Flycatcher</td>\n",
       "      <td>-82.1106</td>\n",
       "      <td>0-3(s)</td>\n",
       "      <td>08:30</td>\n",
       "      <td>Mike Nelson</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>32000</td>\n",
       "      <td>XC135454.wav</td>\n",
       "      <td>1 (mono)</td>\n",
       "      <td>/home/knikaido/work/Cornell-Birdcall-Identific...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>no</td>\n",
       "      <td>aldfly</td>\n",
       "      <td>2 (stereo)</td>\n",
       "      <td>2013-05-27</td>\n",
       "      <td>both</td>\n",
       "      <td>39</td>\n",
       "      <td>XC135455.mp3</td>\n",
       "      <td>both</td>\n",
       "      <td>Alder Flycatcher</td>\n",
       "      <td>...</td>\n",
       "      <td>Empidonax alnorum_Alder Flycatcher</td>\n",
       "      <td>-82.1106</td>\n",
       "      <td>0-3(s)</td>\n",
       "      <td>08:30</td>\n",
       "      <td>Mike Nelson</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>32000</td>\n",
       "      <td>XC135455.wav</td>\n",
       "      <td>1 (mono)</td>\n",
       "      <td>/home/knikaido/work/Cornell-Birdcall-Identific...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.5</td>\n",
       "      <td>no</td>\n",
       "      <td>aldfly</td>\n",
       "      <td>2 (stereo)</td>\n",
       "      <td>2013-05-27</td>\n",
       "      <td>both</td>\n",
       "      <td>33</td>\n",
       "      <td>XC135456.mp3</td>\n",
       "      <td>both</td>\n",
       "      <td>Alder Flycatcher</td>\n",
       "      <td>...</td>\n",
       "      <td>Empidonax alnorum_Alder Flycatcher</td>\n",
       "      <td>-82.1106</td>\n",
       "      <td>0-3(s)</td>\n",
       "      <td>08:30</td>\n",
       "      <td>Mike Nelson</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>32000</td>\n",
       "      <td>XC135456.wav</td>\n",
       "      <td>1 (mono)</td>\n",
       "      <td>/home/knikaido/work/Cornell-Birdcall-Identific...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>no</td>\n",
       "      <td>aldfly</td>\n",
       "      <td>2 (stereo)</td>\n",
       "      <td>2013-05-27</td>\n",
       "      <td>both</td>\n",
       "      <td>36</td>\n",
       "      <td>XC135457.mp3</td>\n",
       "      <td>level</td>\n",
       "      <td>Alder Flycatcher</td>\n",
       "      <td>...</td>\n",
       "      <td>Empidonax alnorum_Alder Flycatcher</td>\n",
       "      <td>-82.1106</td>\n",
       "      <td>0-3(s)</td>\n",
       "      <td>08:30</td>\n",
       "      <td>Mike Nelson</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>32000</td>\n",
       "      <td>XC135457.wav</td>\n",
       "      <td>1 (mono)</td>\n",
       "      <td>/home/knikaido/work/Cornell-Birdcall-Identific...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating playback_used ebird_code    channels        date          pitch  \\\n",
       "0     3.5            no     aldfly    1 (mono)  2013-05-25  Not specified   \n",
       "1     4.0            no     aldfly  2 (stereo)  2013-05-27           both   \n",
       "2     4.0            no     aldfly  2 (stereo)  2013-05-27           both   \n",
       "3     3.5            no     aldfly  2 (stereo)  2013-05-27           both   \n",
       "4     4.0            no     aldfly  2 (stereo)  2013-05-27           both   \n",
       "\n",
       "   duration      filename          speed           species  ...  \\\n",
       "0        25  XC134874.mp3  Not specified  Alder Flycatcher  ...   \n",
       "1        36  XC135454.mp3           both  Alder Flycatcher  ...   \n",
       "2        39  XC135455.mp3           both  Alder Flycatcher  ...   \n",
       "3        33  XC135456.mp3           both  Alder Flycatcher  ...   \n",
       "4        36  XC135457.mp3          level  Alder Flycatcher  ...   \n",
       "\n",
       "                        primary_label longitude         length   time  \\\n",
       "0  Empidonax alnorum_Alder Flycatcher   -92.962  Not specified   8:00   \n",
       "1  Empidonax alnorum_Alder Flycatcher  -82.1106         0-3(s)  08:30   \n",
       "2  Empidonax alnorum_Alder Flycatcher  -82.1106         0-3(s)  08:30   \n",
       "3  Empidonax alnorum_Alder Flycatcher  -82.1106         0-3(s)  08:30   \n",
       "4  Empidonax alnorum_Alder Flycatcher  -82.1106         0-3(s)  08:30   \n",
       "\n",
       "          recordist                                            license  \\\n",
       "0  Jonathon Jongsma        Creative Commons Attribution-ShareAlike 3.0   \n",
       "1       Mike Nelson  Creative Commons Attribution-NonCommercial-Sha...   \n",
       "2       Mike Nelson  Creative Commons Attribution-NonCommercial-Sha...   \n",
       "3       Mike Nelson  Creative Commons Attribution-NonCommercial-Sha...   \n",
       "4       Mike Nelson  Creative Commons Attribution-NonCommercial-Sha...   \n",
       "\n",
       "  resampled_sampling_rate resampled_filename resampled_channels  \\\n",
       "0                   32000       XC134874.wav           1 (mono)   \n",
       "1                   32000       XC135454.wav           1 (mono)   \n",
       "2                   32000       XC135455.wav           1 (mono)   \n",
       "3                   32000       XC135456.wav           1 (mono)   \n",
       "4                   32000       XC135457.wav           1 (mono)   \n",
       "\n",
       "                                           file_path  \n",
       "0  /home/knikaido/work/Cornell-Birdcall-Identific...  \n",
       "1  /home/knikaido/work/Cornell-Birdcall-Identific...  \n",
       "2  /home/knikaido/work/Cornell-Birdcall-Identific...  \n",
       "3  /home/knikaido/work/Cornell-Birdcall-Identific...  \n",
       "4  /home/knikaido/work/Cornell-Birdcall-Identific...  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_list = []\n",
    "for audio_d in TRAIN_RESAMPLED_AUDIO_DIRS:\n",
    "    if not audio_d.exists():\n",
    "        continue\n",
    "    for ebird_d in audio_d.iterdir():\n",
    "        if ebird_d.is_file():\n",
    "            continue\n",
    "        for wav_f in ebird_d.iterdir():\n",
    "            tmp_list.append([ebird_d.name, wav_f.name, wav_f.as_posix()])\n",
    "            \n",
    "train_wav_path_exist = pd.DataFrame(\n",
    "    tmp_list, columns=[\"ebird_code\", \"resampled_filename\", \"file_path\"])\n",
    "\n",
    "del tmp_list\n",
    "\n",
    "train_all = pd.merge(\n",
    "    train, train_wav_path_exist, on=[\"ebird_code\", \"resampled_filename\"], how=\"inner\")\n",
    "\n",
    "print(train.shape)\n",
    "print(train_wav_path_exist.shape)\n",
    "print(train_all.shape)\n",
    "train_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f3713f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learner class(pytorch-lighting)\n",
    "class Learner(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = get_criterion()\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        b_data = batch\n",
    "        output = self.model(b_data['image'])\n",
    "        loss = self.criterion(output, b_data[\"targets\"])\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        b_data = batch\n",
    "        output = self.model(b_data['image'])\n",
    "        loss = self.criterion(output, b_data[\"targets\"])\n",
    "        \n",
    "        clipwise_ousput_np = to_np(output['clipwise_output'])\n",
    "        targets_np = to_np(b_data[\"targets\"])\n",
    "        f1_score_3 = metrics.f1_score(targets_np, clipwise_ousput_np > 0.3, average=\"samples\")\n",
    "        f1_score_5 = metrics.f1_score(targets_np, clipwise_ousput_np > 0.5, average=\"samples\")\n",
    "        # floor lossは現状は無視して良い\n",
    "        self.log(f'Loss/val', loss, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.log(f'F1_03/val', f1_score_3, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.log(f'F1_05/val', f1_score_5, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x for x in outputs]).mean()\n",
    "        print(f'epoch = {self.current_epoch}, loss = {avg_loss}')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = get_optimizer(self.model)\n",
    "        scheduler = get_scheduler(optimizer)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"Loss/val\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cf8de00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_np(input):\n",
    "    return input.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6e29c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data_ in loaders['train']:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1648ffb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msqrt4kaido\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">pleasant-serenity-105</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/sqrt4kaido/BirdCLEF-2021\" target=\"_blank\">https://wandb.ai/sqrt4kaido/BirdCLEF-2021</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/sqrt4kaido/BirdCLEF-2021/runs/31siy1wi\" target=\"_blank\">https://wandb.ai/sqrt4kaido/BirdCLEF-2021/runs/31siy1wi</a><br/>\n",
       "                Run data is saved locally in <code>/home/knikaido/work/BirdCLEF2021/Git/Notebook/29/wandb/run-20210525_005131-31siy1wi</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | TimmSED          | 12.5 M\n",
      "1 | criterion | BCEFocal2WayLoss | 0     \n",
      "-----------------------------------------------\n",
      "8.2 M     Trainable params\n",
      "4.3 M     Non-trainable params\n",
      "12.5 M    Total params\n",
      "50.016    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0, loss = 12.748104095458984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833b946d4b5e4d96a1a588bc52ce4b79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0, loss = 0.00478665204718709\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1, loss = 0.0037473232951015234\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 2, loss = 0.003074358683079481\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 3, loss = 0.00262495712377131\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 4, loss = 0.0023588710464537144\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 5, loss = 0.0020753215067088604\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 6, loss = 0.0018306566635146737\n"
     ]
    }
   ],
   "source": [
    "for i, (trn_idx, val_idx) in enumerate(splitter.split(train_all, y=train[\"primary_label\"])):\n",
    "    if i not in CFG.folds:\n",
    "        continue\n",
    "\n",
    "    trn_df = train_all.loc[trn_idx, :].reset_index(drop=True)\n",
    "    val_df = train_all.loc[val_idx, :].reset_index(drop=True)\n",
    "    \n",
    "    loaders = {\n",
    "        phase: torchdata.DataLoader(\n",
    "            WaveformDataset(\n",
    "                df_,\n",
    "                img_size=CFG.img_size,\n",
    "                waveform_transforms=get_transforms(phase),\n",
    "                period=CFG.period,\n",
    "                validation=(phase == \"valid\")\n",
    "            ),\n",
    "            **CFG.loader_params[phase])  # type: ignore\n",
    "        for phase, df_ in zip([\"train\", \"valid\"], [trn_df, val_df])\n",
    "    }\n",
    "    \n",
    "    model = TimmSED(\n",
    "        base_model_name=CFG.base_model_name,\n",
    "        pretrained=CFG.pretrained,\n",
    "        num_classes=CFG.num_classes,\n",
    "        in_channels=CFG.in_channels)\n",
    "    model_name = model.__class__.__name__\n",
    "    \n",
    "    learner = Learner(model)\n",
    "    \n",
    "    # loggers\n",
    "    RUN_NAME = f'exp{str(CFG.exp_num)}'\n",
    "    wandb.init(project='BirdCLEF-2021', entity='sqrt4kaido', group=RUN_NAME, job_type=RUN_NAME + f'-fold-{i}')\n",
    "    wandb.run.name = RUN_NAME + f'-fold-{i}'\n",
    "    wandb_config = wandb.config\n",
    "    wandb_config.model_name = model_name\n",
    "    wandb.watch(model)\n",
    "    \n",
    "    # callbacks\n",
    "    callbacks = []\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=f'Loss/val',\n",
    "        mode='min',\n",
    "        dirpath=OUTPUT_DIR,\n",
    "        verbose=False,\n",
    "        filename=f'{model_name}-{learner.current_epoch}-{i}')\n",
    "    callbacks.append(checkpoint_callback)\n",
    "\n",
    "#     early_stop_callback = EarlyStopping(\n",
    "#         monitor='Loss/val',\n",
    "#         min_delta=0.00,\n",
    "#         patience=20,\n",
    "#         verbose=True,\n",
    "#         mode='min')\n",
    "#     callbacks.append(early_stop_callback)\n",
    "    \n",
    "    loggers = []\n",
    "    loggers.append(WandbLogger())\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        logger=loggers,\n",
    "        callbacks=callbacks,\n",
    "        max_epochs=CFG.epochs,\n",
    "        default_root_dir=OUTPUT_DIR,\n",
    "        gpus=1,\n",
    "        fast_dev_run=DEBUG,\n",
    "        deterministic=True,\n",
    "        benchmark=True,\n",
    "        )\n",
    "    \n",
    "    trainer.fit(learner, train_dataloader=loaders['train'], val_dataloaders=loaders['valid'])\n",
    "    \n",
    "    trainer.save_checkpoint(OUTPUT_DIR / \"last.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e854c848",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.encoder.state_dict(), './output/last_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f679158",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project='BirdCLEF-2021', entity='sqrt4kaido', group=RUN_NAME, job_type='summary')\n",
    "wandb.run.name = 'summary'\n",
    "# wandb.log({'CV_score': oofs_score})\n",
    "wandb.save('./config_.py')\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdb879b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
