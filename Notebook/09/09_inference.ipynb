{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cultural-federal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import timm\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as torchdata\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from timm.models.layers import SelectAdaptivePool2d\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torchlibrosa.stft import LogmelFilterBank, Spectrogram\n",
    "from torchlibrosa.augmentation import SpecAugmentation\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-proxy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "featured-parallel",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../')\n",
    "import src.utils as utils\n",
    "from config_ import CFG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-south",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "advised-kenya",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSION = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "noted-discount",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SUBMISSION:\n",
    "    DATA_DIR = Path(\"../input/\")\n",
    "    OUTPUT_DIR = Path(\"\")\n",
    "else:\n",
    "    DATA_DIR = Path(\"/home/knikaido/work/BirdCLEF2021/data/\")\n",
    "    OUTPUT_DIR = Path('./output/')\n",
    "MAIN_DATA_DIR = DATA_DIR / 'birdclef-2021'\n",
    "\n",
    "TEST = (len(list((MAIN_DATA_DIR / \"test_soundscapes/\").glob(\"*.ogg\"))) != 0)\n",
    "if TEST:\n",
    "    DATADIR = MAIN_DATA_DIR / \"test_soundscapes/\"\n",
    "else:\n",
    "    DATADIR = MAIN_DATA_DIR / \"train_soundscapes/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "inclusive-pitch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57610_COR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2782_SSW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26709_SSW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42907_SSW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14473_SSW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>54955_SSW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31928_COR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>28933_SSW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>21767_COR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18003_COR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7843_SSW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10534_SSW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11254_COR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>50878_COR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>26746_COR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7954_COR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7019_COR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20152_SSW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>44957_COR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>51010_SSW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       row_id\n",
       "0   57610_COR\n",
       "1    2782_SSW\n",
       "2   26709_SSW\n",
       "3   42907_SSW\n",
       "4   14473_SSW\n",
       "5   54955_SSW\n",
       "6   31928_COR\n",
       "7   28933_SSW\n",
       "8   21767_COR\n",
       "9   18003_COR\n",
       "10   7843_SSW\n",
       "11  10534_SSW\n",
       "12  11254_COR\n",
       "13  50878_COR\n",
       "14  26746_COR\n",
       "15   7954_COR\n",
       "16   7019_COR\n",
       "17  20152_SSW\n",
       "18  44957_COR\n",
       "19  51010_SSW"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_audios = list(DATADIR.glob(\"*.ogg\"))\n",
    "all_audio_ids = [\"_\".join(audio_id.name.split(\"_\")[:2]) for audio_id in all_audios]\n",
    "submission_df = pd.DataFrame({\n",
    "    \"row_id\": all_audio_ids\n",
    "})\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-adoption",
   "metadata": {},
   "source": [
    "## Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "brilliant-massachusetts",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(torchdata.Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, clip: np.ndarray,\n",
    "                 waveform_transforms=None):\n",
    "        self.df = df\n",
    "        self.clip = clip\n",
    "        self.waveform_transforms=waveform_transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        SR = CFG.sample_rate\n",
    "        sample = self.df.loc[idx, :]\n",
    "        row_id = sample.row_id\n",
    "\n",
    "        end_seconds = int(sample.seconds)\n",
    "        start_seconds = int(end_seconds - 5)\n",
    "\n",
    "        start_index = SR * start_seconds\n",
    "        end_index = SR * end_seconds\n",
    "\n",
    "        y = self.clip[start_index:end_index].astype(np.float32)\n",
    "\n",
    "        y = np.nan_to_num(y)\n",
    "\n",
    "        if self.waveform_transforms:\n",
    "            y = self.waveform_transforms(y)\n",
    "\n",
    "        y = np.nan_to_num(y)\n",
    "\n",
    "        return y, row_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "going-amendment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(phase: str):\n",
    "    transforms = CFG.transforms\n",
    "    if transforms is None:\n",
    "        return None\n",
    "    else:\n",
    "        if transforms[phase] is None:\n",
    "            return None\n",
    "        trns_list = []\n",
    "        for trns_conf in transforms[phase]:\n",
    "            trns_name = trns_conf[\"name\"]\n",
    "            trns_params = {} if trns_conf.get(\"params\") is None else \\\n",
    "                trns_conf[\"params\"]\n",
    "            if globals().get(trns_name) is not None:\n",
    "                trns_cls = globals()[trns_name]\n",
    "                trns_list.append(trns_cls(**trns_params))\n",
    "\n",
    "        if len(trns_list) > 0:\n",
    "            return Compose(trns_list)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "def get_waveform_transforms(config: dict, phase: str):\n",
    "    return get_transforms(config, phase)\n",
    "\n",
    "\n",
    "def get_spectrogram_transforms(config: dict, phase: str):\n",
    "    transforms = config.get('spectrogram_transforms')\n",
    "    if transforms is None:\n",
    "        return None\n",
    "    else:\n",
    "        if transforms[phase] is None:\n",
    "            return None\n",
    "        trns_list = []\n",
    "        for trns_conf in transforms[phase]:\n",
    "            trns_name = trns_conf[\"name\"]\n",
    "            trns_params = {} if trns_conf.get(\"params\") is None else \\\n",
    "                trns_conf[\"params\"]\n",
    "            if hasattr(A, trns_name):\n",
    "                trns_cls = A.__getattribute__(trns_name)\n",
    "                trns_list.append(trns_cls(**trns_params))\n",
    "            else:\n",
    "                trns_cls = globals().get(trns_name)\n",
    "                if trns_cls is not None:\n",
    "                    trns_list.append(trns_cls(**trns_params))\n",
    "\n",
    "        if len(trns_list) > 0:\n",
    "            return A.Compose(trns_list, p=1.0)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "class Normalize:\n",
    "    def __call__(self, y: np.ndarray):\n",
    "        max_vol = np.abs(y).max()\n",
    "        y_vol = y * 1 / max_vol\n",
    "        return np.asfortranarray(y_vol)\n",
    "\n",
    "\n",
    "class NewNormalize:\n",
    "    def __call__(self, y: np.ndarray):\n",
    "        y_mm = y - y.mean()\n",
    "        return y_mm / y_mm.abs().max()\n",
    "    \n",
    "class Compose:\n",
    "    def __init__(self, transforms: list):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, y: np.ndarray):\n",
    "        for trns in self.transforms:\n",
    "            y = trns(y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-mandate",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "centered-excellence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layer(layer):\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "    if hasattr(layer, \"bias\"):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "\n",
    "\n",
    "def init_bn(bn):\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.0)\n",
    "\n",
    "\n",
    "def init_weights(model):\n",
    "    classname = model.__class__.__name__\n",
    "    if classname.find(\"Conv2d\") != -1:\n",
    "        nn.init.xavier_uniform_(model.weight, gain=np.sqrt(2))\n",
    "        model.bias.data.fill_(0)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        model.weight.data.normal_(1.0, 0.02)\n",
    "        model.bias.data.fill_(0)\n",
    "    elif classname.find(\"GRU\") != -1:\n",
    "        for weight in model.parameters():\n",
    "            if len(weight.size()) > 1:\n",
    "                nn.init.orghogonal_(weight.data)\n",
    "    elif classname.find(\"Linear\") != -1:\n",
    "        model.weight.data.normal_(0, 0.01)\n",
    "        model.bias.data.zero_()\n",
    "\n",
    "\n",
    "def do_mixup(x: torch.Tensor, mixup_lambda: torch.Tensor):\n",
    "    \"\"\"Mixup x of even indexes (0, 2, 4, ...) with x of odd indexes\n",
    "    (1, 3, 5, ...).\n",
    "    Args:\n",
    "      x: (batch_size * 2, ...)\n",
    "      mixup_lambda: (batch_size * 2,)\n",
    "    Returns:\n",
    "      out: (batch_size, ...)\n",
    "    \"\"\"\n",
    "    out = (x[0::2].transpose(0, -1) * mixup_lambda[0::2] +\n",
    "           x[1::2].transpose(0, -1) * mixup_lambda[1::2]).transpose(0, -1)\n",
    "    return out\n",
    "\n",
    "\n",
    "class Mixup(object):\n",
    "    def __init__(self, mixup_alpha, random_seed=1234):\n",
    "        \"\"\"Mixup coefficient generator.\n",
    "        \"\"\"\n",
    "        self.mixup_alpha = mixup_alpha\n",
    "        self.random_state = np.random.RandomState(random_seed)\n",
    "\n",
    "    def get_lambda(self, batch_size):\n",
    "        \"\"\"Get mixup random coefficients.\n",
    "        Args:\n",
    "          batch_size: int\n",
    "        Returns:\n",
    "          mixup_lambdas: (batch_size,)\n",
    "        \"\"\"\n",
    "        mixup_lambdas = []\n",
    "        for n in range(0, batch_size, 2):\n",
    "            lam = self.random_state.beta(\n",
    "                self.mixup_alpha, self.mixup_alpha, 1)[0]\n",
    "            mixup_lambdas.append(lam)\n",
    "            mixup_lambdas.append(1. - lam)\n",
    "\n",
    "        return torch.from_numpy(np.array(mixup_lambdas, dtype=np.float32))\n",
    "\n",
    "\n",
    "def interpolate(x: torch.Tensor, ratio: int):\n",
    "    \"\"\"Interpolate data in time domain. This is used to compensate the\n",
    "    resolution reduction in downsampling of a CNN.\n",
    "    Args:\n",
    "      x: (batch_size, time_steps, classes_num)\n",
    "      ratio: int, ratio to interpolate\n",
    "    Returns:\n",
    "      upsampled: (batch_size, time_steps * ratio, classes_num)\n",
    "    \"\"\"\n",
    "    (batch_size, time_steps, classes_num) = x.shape\n",
    "    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n",
    "    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n",
    "    return upsampled\n",
    "\n",
    "\n",
    "def pad_framewise_output(framewise_output: torch.Tensor, frames_num: int):\n",
    "    \"\"\"Pad framewise_output to the same length as input frames. The pad value\n",
    "    is the same as the value of the last frame.\n",
    "    Args:\n",
    "      framewise_output: (batch_size, frames_num, classes_num)\n",
    "      frames_num: int, number of frames to pad\n",
    "    Outputs:\n",
    "      output: (batch_size, frames_num, classes_num)\n",
    "    \"\"\"\n",
    "    output = F.interpolate(\n",
    "        framewise_output.unsqueeze(1),\n",
    "        size=(frames_num, framewise_output.size(2)),\n",
    "        align_corners=True,\n",
    "        mode=\"bilinear\").squeeze(1)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def gem(x: torch.Tensor, p=3, eps=1e-6):\n",
    "    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1. / p)\n",
    "\n",
    "\n",
    "class GeM(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.p = nn.Parameter(torch.ones(1) * p)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return gem(x, p=self.p, eps=self.eps)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + f\"(p={self.p.data.tolist()[0]:.4f}, eps={self.eps})\"\n",
    "\n",
    "\n",
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return x\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "criminal-violence",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TimmSED(nn.Module):\n",
    "    def __init__(self, base_model_name: str, pretrained=False, num_classes=24, in_channels=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.melspectrogramer = torchaudio.transforms.MelSpectrogram(sample_rate=CFG.sample_rate, n_fft=CFG.n_fft, win_length=CFG.n_fft, \n",
    "                                                                     hop_length=CFG.hop_length, f_min=CFG.fmin, \n",
    "                                                                     f_max=CFG.fmax, n_mels=CFG.n_mels, power=1.0)\n",
    "        self.amplituder = torchaudio.transforms.AmplitudeToDB()\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(CFG.n_mels)\n",
    "\n",
    "        base_model = timm.create_model(\n",
    "            base_model_name, pretrained=pretrained, in_chans=in_channels)\n",
    "        layers = list(base_model.children())[:-2]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "\n",
    "        if hasattr(base_model, \"fc\"):\n",
    "            in_features = base_model.fc.in_features\n",
    "        else:\n",
    "            in_features = base_model.classifier.in_features\n",
    "        self.fc1 = nn.Linear(in_features, in_features, bias=True)\n",
    "        self.att_block = AttBlockV2(\n",
    "            in_features, num_classes, activation=\"sigmoid\")\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_layer(self.fc1)\n",
    "        init_bn(self.bn0)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # (batch_size, 1, mel_bins, time_steps)\n",
    "        \n",
    "        x = self.melspectrogramer(input)\n",
    "        x = self.amplituder(x).unsqueeze(1)\n",
    "        frames_num = x.shape[3]\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "#         if self.training:\n",
    "#             x = self.spec_augmenter(x)\n",
    "\n",
    "        # (batch_size, channels, freq, frames)\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # (batch_size, channels, frames)\n",
    "        x = torch.mean(x, dim=2)\n",
    "\n",
    "        # channel smoothing\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n",
    "        logit = torch.sum(norm_att * self.att_block.cla(x), dim=2)\n",
    "        segmentwise_logit = self.att_block.cla(x).transpose(1, 2)\n",
    "        segmentwise_output = segmentwise_output.transpose(1, 2)\n",
    "\n",
    "        interpolate_ratio = frames_num // segmentwise_output.size(1)\n",
    "\n",
    "        # Get framewise output\n",
    "        framewise_output = interpolate(segmentwise_output,\n",
    "                                       interpolate_ratio)\n",
    "        framewise_output = pad_framewise_output(framewise_output, frames_num)\n",
    "\n",
    "        framewise_logit = interpolate(segmentwise_logit, interpolate_ratio)\n",
    "        framewise_logit = pad_framewise_output(framewise_logit, frames_num)\n",
    "\n",
    "        output_dict = {\n",
    "            \"framewise_output\": framewise_output,\n",
    "            \"segmentwise_output\": segmentwise_output,\n",
    "            \"logit\": logit,\n",
    "            \"framewise_logit\": framewise_logit,\n",
    "            \"clipwise_output\": clipwise_output\n",
    "        }\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-lying",
   "metadata": {},
   "source": [
    "## Get Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "threatened-rider",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_for_inference(model, path: Path):\n",
    "    if not torch.cuda.is_available():\n",
    "        ckpt = torch.load(path, map_location=\"cpu\")\n",
    "    else:\n",
    "        ckpt = torch.load(path)\n",
    "    model.load_state_dict(ckpt[\"state_dict\"])\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "operational-square",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_for_clip(test_df: pd.DataFrame, \n",
    "                        clip: np.ndarray, \n",
    "                        model, \n",
    "                        threshold=0.5):\n",
    "\n",
    "    dataset = TestDataset(df=test_df, \n",
    "                          clip=clip,\n",
    "                          waveform_transforms=get_transforms(phase=\"test\"))\n",
    "    loader = torchdata.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model.eval()\n",
    "    prediction_dict = {}\n",
    "    for image, row_id in tqdm(loader):\n",
    "        row_id = row_id[0]\n",
    "        image = image.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prediction = model(image)\n",
    "            proba = prediction[\"clipwise_output\"].detach().cpu().numpy().reshape(-1)\n",
    "\n",
    "        events = proba >= threshold\n",
    "        labels = np.argwhere(events).reshape(-1).tolist()\n",
    "\n",
    "        if len(labels) == 0:\n",
    "            prediction_dict[row_id] = \"nocall\"\n",
    "        else:\n",
    "            labels_str_list = list(map(lambda x: CFG.target_columns[x], labels))\n",
    "            label_string = \" \".join(labels_str_list)\n",
    "            prediction_dict[row_id] = label_string\n",
    "    return prediction_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "crazy-texture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learner class(pytorch-lighting)\n",
    "class Learner(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "british-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(test_audios,\n",
    "               weights_path: Path,\n",
    "               threshold=0.5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = TimmSED(\n",
    "        base_model_name=CFG.base_model_name,\n",
    "        pretrained=CFG.pretrained,\n",
    "        num_classes=CFG.num_classes,\n",
    "        in_channels=CFG.in_channels)\n",
    "#     model = prepare_model_for_inference(model, weights_path).to(device)\n",
    "    checkpoint = torch.load(weights_path)\n",
    "    Learner(model).load_state_dict(checkpoint['state_dict'])\n",
    "    model = model.to(device)\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    prediction_dfs = []\n",
    "    for audio_path in test_audios:\n",
    "#         with timer(f\"Loading {str(audio_path)}\", logger):\n",
    "        clip, _ = sf.read(audio_path)\n",
    "\n",
    "        seconds = []\n",
    "        row_ids = []\n",
    "        for second in range(5, 605, 5):\n",
    "            row_id = \"_\".join(audio_path.name.split(\"_\")[:2]) + f\"_{second}\"\n",
    "            seconds.append(second)\n",
    "            row_ids.append(row_id)\n",
    "            \n",
    "        test_df = pd.DataFrame({\n",
    "            \"row_id\": row_ids,\n",
    "            \"seconds\": seconds\n",
    "        })\n",
    "#         with timer(f\"Prediction on {audio_path}\", logger):\n",
    "        prediction_dict = prediction_for_clip(test_df,\n",
    "                                                  clip=clip,\n",
    "                                                  model=model,\n",
    "                                                  threshold=threshold)\n",
    "        row_id = list(prediction_dict.keys())\n",
    "        birds = list(prediction_dict.values())\n",
    "        prediction_df = pd.DataFrame({\n",
    "            \"row_id\": row_id,\n",
    "            \"birds\": birds\n",
    "        })\n",
    "        prediction_dfs.append(prediction_df)\n",
    "    \n",
    "    prediction_df = pd.concat(prediction_dfs, axis=0, sort=False).reset_index(drop=True)\n",
    "    return prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-wages",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "serial-chair",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:01<00:00, 103.37it/s]\n",
      "100%|██████████| 120/120 [00:01<00:00, 103.35it/s]\n",
      "100%|██████████| 120/120 [00:01<00:00, 103.45it/s]\n",
      "100%|██████████| 120/120 [00:01<00:00, 103.25it/s]\n",
      "100%|██████████| 120/120 [00:01<00:00, 103.27it/s]\n",
      "100%|██████████| 120/120 [00:01<00:00, 103.25it/s]\n",
      "100%|██████████| 120/120 [00:01<00:00, 103.35it/s]\n",
      "100%|██████████| 120/120 [00:01<00:00, 103.42it/s]\n",
      "100%|██████████| 120/120 [00:01<00:00, 103.37it/s]\n",
      "100%|██████████| 120/120 [00:01<00:00, 103.31it/s]\n",
      "100%|██████████| 120/120 [00:01<00:00, 103.37it/s]\n",
      "100%|██████████| 120/120 [00:01<00:00, 103.42it/s]\n",
      "100%|██████████| 120/120 [00:01<00:00, 103.32it/s]\n",
      "100%|██████████| 120/120 [00:01<00:00, 102.90it/s]\n",
      "100%|██████████| 120/120 [00:01<00:00, 103.09it/s]\n",
      "100%|██████████| 120/120 [00:01<00:00, 103.22it/s]\n",
      "100%|██████████| 120/120 [00:01<00:00, 103.10it/s]\n",
      "100%|██████████| 120/120 [00:01<00:00, 103.14it/s]\n",
      "100%|██████████| 120/120 [00:01<00:00, 103.22it/s]\n",
      "100%|██████████| 120/120 [00:01<00:00, 103.25it/s]\n"
     ]
    }
   ],
   "source": [
    "weights_path = Path(\"./output/TimmSED-0-0.ckpt\")\n",
    "submission = prediction(test_audios=all_audios,\n",
    "                        weights_path=weights_path,\n",
    "                        threshold=0.3)\n",
    "submission.to_csv(OUTPUT_DIR / \"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "blocked-habitat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6384027777777707\n"
     ]
    }
   ],
   "source": [
    "if not SUBMISSION and not TEST:\n",
    "    sys.path.append('../../')\n",
    "    import src.row_wise_micro_averaged_f1 as mi_f1\n",
    "    \n",
    "    submission = pd.read_csv(OUTPUT_DIR / \"submission.csv\")\n",
    "    ans = pd.read_csv(MAIN_DATA_DIR / 'train_soundscape_labels.csv')\n",
    "    \n",
    "    submission = pd.merge(submission, ans, how='inner', on='row_id')\n",
    "    \n",
    "    f1_score = mi_f1.row_wise_micro_averaged_f1_score(list(submission['birds_y']), list(submission['birds_x']))\n",
    "    print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-threat",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
