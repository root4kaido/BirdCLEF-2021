{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4d84e325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import timm\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as torchdata\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from timm.models.layers import SelectAdaptivePool2d\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torchlibrosa.stft import LogmelFilterBank, Spectrogram\n",
    "from torchlibrosa.augmentation import SpecAugmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ff0a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7b07cb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../')\n",
    "import src.utils as utils\n",
    "from config_ import CFG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ff4818",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "94c6f7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSION = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cc2a6c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SUBMISSION:\n",
    "    DATA_DIR = Path(\"../input/\")\n",
    "    OUTPUT_DIR = Path(\"\")\n",
    "else:\n",
    "    DATA_DIR = Path(\"/home/knikaido/work/BirdCLEF2021/data/\")\n",
    "    OUTPUT_DIR = Path('./output/')\n",
    "MAIN_DATA_DIR = DATA_DIR / 'birdclef-2021'\n",
    "\n",
    "TEST = (len(list((MAIN_DATA_DIR / \"test_soundscapes/\").glob(\"*.ogg\"))) != 0)\n",
    "if TEST:\n",
    "    DATADIR = MAIN_DATA_DIR / \"test_soundscapes/\"\n",
    "else:\n",
    "    DATADIR = MAIN_DATA_DIR / \"train_soundscapes/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "18d5a532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57610_COR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2782_SSW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26709_SSW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42907_SSW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14473_SSW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>54955_SSW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31928_COR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>28933_SSW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>21767_COR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18003_COR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7843_SSW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10534_SSW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11254_COR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>50878_COR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>26746_COR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7954_COR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7019_COR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20152_SSW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>44957_COR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>51010_SSW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       row_id\n",
       "0   57610_COR\n",
       "1    2782_SSW\n",
       "2   26709_SSW\n",
       "3   42907_SSW\n",
       "4   14473_SSW\n",
       "5   54955_SSW\n",
       "6   31928_COR\n",
       "7   28933_SSW\n",
       "8   21767_COR\n",
       "9   18003_COR\n",
       "10   7843_SSW\n",
       "11  10534_SSW\n",
       "12  11254_COR\n",
       "13  50878_COR\n",
       "14  26746_COR\n",
       "15   7954_COR\n",
       "16   7019_COR\n",
       "17  20152_SSW\n",
       "18  44957_COR\n",
       "19  51010_SSW"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_audios = list(DATADIR.glob(\"*.ogg\"))\n",
    "all_audio_ids = [\"_\".join(audio_id.name.split(\"_\")[:2]) for audio_id in all_audios]\n",
    "submission_df = pd.DataFrame({\n",
    "    \"row_id\": all_audio_ids\n",
    "})\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba524daa",
   "metadata": {},
   "source": [
    "## Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "db430ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(torchdata.Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, clip: np.ndarray,\n",
    "                 waveform_transforms=None):\n",
    "        self.df = df\n",
    "        self.clip = clip\n",
    "        self.waveform_transforms=waveform_transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        SR = CFG.sample_rate\n",
    "        sample = self.df.loc[idx, :]\n",
    "        row_id = sample.row_id\n",
    "\n",
    "        end_seconds = int(sample.seconds)\n",
    "        start_seconds = int(end_seconds - 5)\n",
    "\n",
    "        start_index = SR * start_seconds\n",
    "        end_index = SR * end_seconds\n",
    "\n",
    "        y = self.clip[start_index:end_index].astype(np.float32)\n",
    "\n",
    "        y = np.nan_to_num(y)\n",
    "\n",
    "        if self.waveform_transforms:\n",
    "            y = self.waveform_transforms(y)\n",
    "\n",
    "        y = np.nan_to_num(y)\n",
    "\n",
    "        return y, row_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e295d7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(phase: str):\n",
    "    transforms = CFG.transforms\n",
    "    if transforms is None:\n",
    "        return None\n",
    "    else:\n",
    "        if transforms[phase] is None:\n",
    "            return None\n",
    "        trns_list = []\n",
    "        for trns_conf in transforms[phase]:\n",
    "            trns_name = trns_conf[\"name\"]\n",
    "            trns_params = {} if trns_conf.get(\"params\") is None else \\\n",
    "                trns_conf[\"params\"]\n",
    "            if globals().get(trns_name) is not None:\n",
    "                trns_cls = globals()[trns_name]\n",
    "                trns_list.append(trns_cls(**trns_params))\n",
    "\n",
    "        if len(trns_list) > 0:\n",
    "            return Compose(trns_list)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "def get_waveform_transforms(config: dict, phase: str):\n",
    "    return get_transforms(config, phase)\n",
    "\n",
    "\n",
    "def get_spectrogram_transforms(config: dict, phase: str):\n",
    "    transforms = config.get('spectrogram_transforms')\n",
    "    if transforms is None:\n",
    "        return None\n",
    "    else:\n",
    "        if transforms[phase] is None:\n",
    "            return None\n",
    "        trns_list = []\n",
    "        for trns_conf in transforms[phase]:\n",
    "            trns_name = trns_conf[\"name\"]\n",
    "            trns_params = {} if trns_conf.get(\"params\") is None else \\\n",
    "                trns_conf[\"params\"]\n",
    "            if hasattr(A, trns_name):\n",
    "                trns_cls = A.__getattribute__(trns_name)\n",
    "                trns_list.append(trns_cls(**trns_params))\n",
    "            else:\n",
    "                trns_cls = globals().get(trns_name)\n",
    "                if trns_cls is not None:\n",
    "                    trns_list.append(trns_cls(**trns_params))\n",
    "\n",
    "        if len(trns_list) > 0:\n",
    "            return A.Compose(trns_list, p=1.0)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "class Normalize:\n",
    "    def __call__(self, y: np.ndarray):\n",
    "        max_vol = np.abs(y).max()\n",
    "        y_vol = y * 1 / max_vol\n",
    "        return np.asfortranarray(y_vol)\n",
    "\n",
    "\n",
    "class NewNormalize:\n",
    "    def __call__(self, y: np.ndarray):\n",
    "        y_mm = y - y.mean()\n",
    "        return y_mm / y_mm.abs().max()\n",
    "    \n",
    "class Compose:\n",
    "    def __init__(self, transforms: list):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, y: np.ndarray):\n",
    "        for trns in self.transforms:\n",
    "            y = trns(y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7625e9f3",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "88096970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layer(layer):\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "    if hasattr(layer, \"bias\"):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "\n",
    "\n",
    "def init_bn(bn):\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.0)\n",
    "\n",
    "\n",
    "def init_weights(model):\n",
    "    classname = model.__class__.__name__\n",
    "    if classname.find(\"Conv2d\") != -1:\n",
    "        nn.init.xavier_uniform_(model.weight, gain=np.sqrt(2))\n",
    "        model.bias.data.fill_(0)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        model.weight.data.normal_(1.0, 0.02)\n",
    "        model.bias.data.fill_(0)\n",
    "    elif classname.find(\"GRU\") != -1:\n",
    "        for weight in model.parameters():\n",
    "            if len(weight.size()) > 1:\n",
    "                nn.init.orghogonal_(weight.data)\n",
    "    elif classname.find(\"Linear\") != -1:\n",
    "        model.weight.data.normal_(0, 0.01)\n",
    "        model.bias.data.zero_()\n",
    "\n",
    "\n",
    "def do_mixup(x: torch.Tensor, mixup_lambda: torch.Tensor):\n",
    "    \"\"\"Mixup x of even indexes (0, 2, 4, ...) with x of odd indexes\n",
    "    (1, 3, 5, ...).\n",
    "    Args:\n",
    "      x: (batch_size * 2, ...)\n",
    "      mixup_lambda: (batch_size * 2,)\n",
    "    Returns:\n",
    "      out: (batch_size, ...)\n",
    "    \"\"\"\n",
    "    out = (x[0::2].transpose(0, -1) * mixup_lambda[0::2] +\n",
    "           x[1::2].transpose(0, -1) * mixup_lambda[1::2]).transpose(0, -1)\n",
    "    return out\n",
    "\n",
    "\n",
    "class Mixup(object):\n",
    "    def __init__(self, mixup_alpha, random_seed=1234):\n",
    "        \"\"\"Mixup coefficient generator.\n",
    "        \"\"\"\n",
    "        self.mixup_alpha = mixup_alpha\n",
    "        self.random_state = np.random.RandomState(random_seed)\n",
    "\n",
    "    def get_lambda(self, batch_size):\n",
    "        \"\"\"Get mixup random coefficients.\n",
    "        Args:\n",
    "          batch_size: int\n",
    "        Returns:\n",
    "          mixup_lambdas: (batch_size,)\n",
    "        \"\"\"\n",
    "        mixup_lambdas = []\n",
    "        for n in range(0, batch_size, 2):\n",
    "            lam = self.random_state.beta(\n",
    "                self.mixup_alpha, self.mixup_alpha, 1)[0]\n",
    "            mixup_lambdas.append(lam)\n",
    "            mixup_lambdas.append(1. - lam)\n",
    "\n",
    "        return torch.from_numpy(np.array(mixup_lambdas, dtype=np.float32))\n",
    "\n",
    "\n",
    "def interpolate(x: torch.Tensor, ratio: int):\n",
    "    \"\"\"Interpolate data in time domain. This is used to compensate the\n",
    "    resolution reduction in downsampling of a CNN.\n",
    "    Args:\n",
    "      x: (batch_size, time_steps, classes_num)\n",
    "      ratio: int, ratio to interpolate\n",
    "    Returns:\n",
    "      upsampled: (batch_size, time_steps * ratio, classes_num)\n",
    "    \"\"\"\n",
    "    (batch_size, time_steps, classes_num) = x.shape\n",
    "    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n",
    "    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n",
    "    return upsampled\n",
    "\n",
    "\n",
    "def pad_framewise_output(framewise_output: torch.Tensor, frames_num: int):\n",
    "    \"\"\"Pad framewise_output to the same length as input frames. The pad value\n",
    "    is the same as the value of the last frame.\n",
    "    Args:\n",
    "      framewise_output: (batch_size, frames_num, classes_num)\n",
    "      frames_num: int, number of frames to pad\n",
    "    Outputs:\n",
    "      output: (batch_size, frames_num, classes_num)\n",
    "    \"\"\"\n",
    "    output = F.interpolate(\n",
    "        framewise_output.unsqueeze(1),\n",
    "        size=(frames_num, framewise_output.size(2)),\n",
    "        align_corners=True,\n",
    "        mode=\"bilinear\").squeeze(1)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def gem(x: torch.Tensor, p=3, eps=1e-6):\n",
    "    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1. / p)\n",
    "\n",
    "\n",
    "class GeM(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.p = nn.Parameter(torch.ones(1) * p)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return gem(x, p=self.p, eps=self.eps)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + f\"(p={self.p.data.tolist()[0]:.4f}, eps={self.eps})\"\n",
    "\n",
    "\n",
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return x\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c3d13a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TimmSED(nn.Module):\n",
    "    def __init__(self, base_model_name: str, pretrained=False, num_classes=24, in_channels=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = Spectrogram(n_fft=CFG.n_fft, hop_length=CFG.hop_length,\n",
    "                                                 win_length=CFG.n_fft, window=\"hann\", center=True, pad_mode=\"reflect\",\n",
    "                                                 freeze_parameters=True)\n",
    "\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = LogmelFilterBank(sr=CFG.sample_rate, n_fft=CFG.n_fft,\n",
    "                                                 n_mels=CFG.n_mels, fmin=CFG.fmin, fmax=CFG.fmax, ref=1.0, amin=1e-10, top_db=None,\n",
    "                                                 freeze_parameters=True)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(CFG.n_mels)\n",
    "\n",
    "        base_model = timm.create_model(\n",
    "            base_model_name, pretrained=pretrained, in_chans=in_channels)\n",
    "        layers = list(base_model.children())[:-2]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "\n",
    "        if hasattr(base_model, \"fc\"):\n",
    "            in_features = base_model.fc.in_features\n",
    "        else:\n",
    "            in_features = base_model.classifier.in_features\n",
    "        self.fc1 = nn.Linear(in_features, in_features, bias=True)\n",
    "        self.att_block = AttBlockV2(\n",
    "            in_features, num_classes, activation=\"sigmoid\")\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_layer(self.fc1)\n",
    "        init_bn(self.bn0)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # (batch_size, 1, time_steps, freq_bins)\n",
    "        x = self.spectrogram_extractor(input)\n",
    "        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n",
    "        frames_num = x.shape[2]\n",
    "\n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "\n",
    "        x = x.transpose(2, 3)\n",
    "        # (batch_size, channels, freq, frames)\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # (batch_size, channels, frames)\n",
    "        x = torch.mean(x, dim=2)\n",
    "\n",
    "        # channel smoothing\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n",
    "        logit = torch.sum(norm_att * self.att_block.cla(x), dim=2)\n",
    "        segmentwise_logit = self.att_block.cla(x).transpose(1, 2)\n",
    "        segmentwise_output = segmentwise_output.transpose(1, 2)\n",
    "\n",
    "        interpolate_ratio = frames_num // segmentwise_output.size(1)\n",
    "\n",
    "        # Get framewise output\n",
    "        framewise_output = interpolate(segmentwise_output,\n",
    "                                       interpolate_ratio)\n",
    "        framewise_output = pad_framewise_output(framewise_output, frames_num)\n",
    "\n",
    "        framewise_logit = interpolate(segmentwise_logit, interpolate_ratio)\n",
    "        framewise_logit = pad_framewise_output(framewise_logit, frames_num)\n",
    "\n",
    "        output_dict = {\n",
    "            \"framewise_output\": framewise_output,\n",
    "            \"segmentwise_output\": segmentwise_output,\n",
    "            \"logit\": logit,\n",
    "            \"framewise_logit\": framewise_logit,\n",
    "            \"clipwise_output\": clipwise_output\n",
    "        }\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eff895",
   "metadata": {},
   "source": [
    "## Get Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "37120aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_for_inference(model, path: Path):\n",
    "    if not torch.cuda.is_available():\n",
    "        ckpt = torch.load(path, map_location=\"cpu\")\n",
    "    else:\n",
    "        ckpt = torch.load(path)\n",
    "    model.load_state_dict(ckpt[\"state_dict\"])\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e96043d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learner class(pytorch-lighting)\n",
    "class Learner(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef36828f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "86cc404a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57610_COR</td>\n",
       "      <td>/home/knikaido/work/BirdCLEF2021/data/birdclef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2782_SSW</td>\n",
       "      <td>/home/knikaido/work/BirdCLEF2021/data/birdclef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26709_SSW</td>\n",
       "      <td>/home/knikaido/work/BirdCLEF2021/data/birdclef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42907_SSW</td>\n",
       "      <td>/home/knikaido/work/BirdCLEF2021/data/birdclef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14473_SSW</td>\n",
       "      <td>/home/knikaido/work/BirdCLEF2021/data/birdclef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>54955_SSW</td>\n",
       "      <td>/home/knikaido/work/BirdCLEF2021/data/birdclef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31928_COR</td>\n",
       "      <td>/home/knikaido/work/BirdCLEF2021/data/birdclef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>28933_SSW</td>\n",
       "      <td>/home/knikaido/work/BirdCLEF2021/data/birdclef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>21767_COR</td>\n",
       "      <td>/home/knikaido/work/BirdCLEF2021/data/birdclef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18003_COR</td>\n",
       "      <td>/home/knikaido/work/BirdCLEF2021/data/birdclef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7843_SSW</td>\n",
       "      <td>/home/knikaido/work/BirdCLEF2021/data/birdclef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10534_SSW</td>\n",
       "      <td>/home/knikaido/work/BirdCLEF2021/data/birdclef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11254_COR</td>\n",
       "      <td>/home/knikaido/work/BirdCLEF2021/data/birdclef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>50878_COR</td>\n",
       "      <td>/home/knikaido/work/BirdCLEF2021/data/birdclef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>26746_COR</td>\n",
       "      <td>/home/knikaido/work/BirdCLEF2021/data/birdclef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7954_COR</td>\n",
       "      <td>/home/knikaido/work/BirdCLEF2021/data/birdclef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7019_COR</td>\n",
       "      <td>/home/knikaido/work/BirdCLEF2021/data/birdclef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20152_SSW</td>\n",
       "      <td>/home/knikaido/work/BirdCLEF2021/data/birdclef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>44957_COR</td>\n",
       "      <td>/home/knikaido/work/BirdCLEF2021/data/birdclef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>51010_SSW</td>\n",
       "      <td>/home/knikaido/work/BirdCLEF2021/data/birdclef...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       row_id                                               path\n",
       "0   57610_COR  /home/knikaido/work/BirdCLEF2021/data/birdclef...\n",
       "1    2782_SSW  /home/knikaido/work/BirdCLEF2021/data/birdclef...\n",
       "2   26709_SSW  /home/knikaido/work/BirdCLEF2021/data/birdclef...\n",
       "3   42907_SSW  /home/knikaido/work/BirdCLEF2021/data/birdclef...\n",
       "4   14473_SSW  /home/knikaido/work/BirdCLEF2021/data/birdclef...\n",
       "5   54955_SSW  /home/knikaido/work/BirdCLEF2021/data/birdclef...\n",
       "6   31928_COR  /home/knikaido/work/BirdCLEF2021/data/birdclef...\n",
       "7   28933_SSW  /home/knikaido/work/BirdCLEF2021/data/birdclef...\n",
       "8   21767_COR  /home/knikaido/work/BirdCLEF2021/data/birdclef...\n",
       "9   18003_COR  /home/knikaido/work/BirdCLEF2021/data/birdclef...\n",
       "10   7843_SSW  /home/knikaido/work/BirdCLEF2021/data/birdclef...\n",
       "11  10534_SSW  /home/knikaido/work/BirdCLEF2021/data/birdclef...\n",
       "12  11254_COR  /home/knikaido/work/BirdCLEF2021/data/birdclef...\n",
       "13  50878_COR  /home/knikaido/work/BirdCLEF2021/data/birdclef...\n",
       "14  26746_COR  /home/knikaido/work/BirdCLEF2021/data/birdclef...\n",
       "15   7954_COR  /home/knikaido/work/BirdCLEF2021/data/birdclef...\n",
       "16   7019_COR  /home/knikaido/work/BirdCLEF2021/data/birdclef...\n",
       "17  20152_SSW  /home/knikaido/work/BirdCLEF2021/data/birdclef...\n",
       "18  44957_COR  /home/knikaido/work/BirdCLEF2021/data/birdclef...\n",
       "19  51010_SSW  /home/knikaido/work/BirdCLEF2021/data/birdclef..."
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df['path'] = all_audios\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d17200d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_for_clip(audio_id: str, \n",
    "                        clip: np.ndarray, \n",
    "                        model, \n",
    "                        threshold=0.5):\n",
    "    audios = []\n",
    "    y = clip.astype(np.float32)\n",
    "    len_y = len(y)\n",
    "    start = 0\n",
    "    end = CFG.period * CFG.sample_rate\n",
    "    while True:\n",
    "        y_batch = y[start:end].astype(np.float32)\n",
    "        if len(y_batch) != CFG.period * CFG.sample_rate:\n",
    "            y_pad = np.zeros(CFG.period * CFG.sample_rate, dtype=np.float32)\n",
    "            y_pad[:len(y_batch)] = y_batch\n",
    "            audios.append(y_pad)\n",
    "            break\n",
    "        start = end\n",
    "        end += CFG.period * CFG.sample_rate\n",
    "        audios.append(y_batch)\n",
    "        \n",
    "    array = np.asarray(audios)\n",
    "    tensors = torch.from_numpy(array)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model.eval()\n",
    "    estimated_event_list = []\n",
    "    global_time = 0.0\n",
    "    for image in tqdm(tensors):\n",
    "        image = image.view(1, image.size(0))\n",
    "        image = image.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prediction = model(image)\n",
    "            framewise_outputs = prediction[\"framewise_output\"].detach(\n",
    "                ).cpu().numpy()[0]\n",
    "                \n",
    "        thresholded = framewise_outputs >= threshold\n",
    "\n",
    "        for target_idx in range(thresholded.shape[1]):\n",
    "            if thresholded[:, target_idx].mean() == 0:\n",
    "                pass\n",
    "            else:\n",
    "                detected = np.argwhere(thresholded[:, target_idx]).reshape(-1)\n",
    "                head_idx = 0\n",
    "                tail_idx = 0\n",
    "                while True:\n",
    "                    if (tail_idx + 1 == len(detected)) or (\n",
    "                            detected[tail_idx + 1] - \n",
    "                            detected[tail_idx] != 1):\n",
    "                        onset = 0.01 * detected[\n",
    "                            head_idx] + global_time\n",
    "                        offset = 0.01 * detected[\n",
    "                            tail_idx] + global_time\n",
    "                        onset_idx = detected[head_idx]\n",
    "                        offset_idx = detected[tail_idx]\n",
    "                        max_confidence = framewise_outputs[\n",
    "                            onset_idx:offset_idx, target_idx].max()\n",
    "                        mean_confidence = framewise_outputs[\n",
    "                            onset_idx:offset_idx, target_idx].mean()\n",
    "                        estimated_event = {\n",
    "                            \"audio_id\": audio_id,\n",
    "                            \"ebird_code\": CFG.target_columns[target_idx],\n",
    "                            \"onset\": onset,\n",
    "                            \"offset\": offset,\n",
    "                            \"max_confidence\": max_confidence,\n",
    "                            \"mean_confidence\": mean_confidence\n",
    "                        }\n",
    "                        estimated_event_list.append(estimated_event)\n",
    "                        head_idx = tail_idx + 1\n",
    "                        tail_idx = tail_idx + 1\n",
    "                        if head_idx >= len(detected):\n",
    "                            break\n",
    "                    else:\n",
    "                        tail_idx += 1\n",
    "        global_time += 5\n",
    "        \n",
    "    prediction_df = pd.DataFrame(estimated_event_list)\n",
    "    return prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8878e159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(test_df,\n",
    "               weights_path: Path,\n",
    "               threshold=0.5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = TimmSED(\n",
    "        base_model_name=CFG.base_model_name,\n",
    "        pretrained=CFG.pretrained,\n",
    "        num_classes=CFG.num_classes,\n",
    "        in_channels=CFG.in_channels)\n",
    "#     model = prepare_model_for_inference(model, weights_path).to(device)\n",
    "    checkpoint = torch.load(weights_path)\n",
    "    Learner(model).load_state_dict(checkpoint['state_dict'])\n",
    "    model = model.to(device)\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    prediction_dfs = []\n",
    "    for i, audio_path in enumerate(test_df['path']):\n",
    "        clip, _ = sf.read(audio_path)\n",
    "        \n",
    "        prediction_df = prediction_for_clip(test_df['row_id'][i],\n",
    "                                            clip=clip,\n",
    "                                            model=model,\n",
    "                                            threshold=threshold)\n",
    "\n",
    "        prediction_dfs.append(prediction_df)\n",
    "    \n",
    "    prediction_df = pd.concat(prediction_dfs, axis=0, sort=False).reset_index(drop=True)\n",
    "    return prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5bd84585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61/61 [00:00<00:00, 83.34it/s]\n",
      "100%|██████████| 61/61 [00:00<00:00, 83.36it/s]\n",
      "100%|██████████| 61/61 [00:00<00:00, 83.50it/s]\n",
      "100%|██████████| 61/61 [00:00<00:00, 83.61it/s]\n",
      "100%|██████████| 61/61 [00:00<00:00, 83.62it/s]\n",
      "100%|██████████| 61/61 [00:00<00:00, 83.49it/s]\n",
      "100%|██████████| 61/61 [00:00<00:00, 83.70it/s]\n",
      "100%|██████████| 61/61 [00:00<00:00, 83.56it/s]\n",
      "100%|██████████| 61/61 [00:00<00:00, 83.63it/s]\n",
      "100%|██████████| 61/61 [00:00<00:00, 83.58it/s]\n",
      "100%|██████████| 61/61 [00:00<00:00, 83.59it/s]\n",
      "100%|██████████| 61/61 [00:00<00:00, 83.57it/s]\n",
      "100%|██████████| 61/61 [00:00<00:00, 83.35it/s]\n",
      "100%|██████████| 61/61 [00:00<00:00, 83.13it/s]\n",
      "100%|██████████| 61/61 [00:00<00:00, 83.55it/s]\n",
      "100%|██████████| 61/61 [00:00<00:00, 83.43it/s]\n",
      "100%|██████████| 61/61 [00:00<00:00, 83.40it/s]\n",
      "100%|██████████| 61/61 [00:00<00:00, 83.58it/s]\n",
      "100%|██████████| 61/61 [00:00<00:00, 83.41it/s]\n",
      "100%|██████████| 61/61 [00:00<00:00, 83.27it/s]\n"
     ]
    }
   ],
   "source": [
    "weights_path = Path(\"./output/TimmSED-0-0.ckpt\")\n",
    "submission = prediction(test_df=submission_df,\n",
    "                        weights_path=weights_path,\n",
    "                        threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a80f8b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b773855e",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = submission[submission['offset'] - submission['onset'] > 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "97e6f31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "\n",
    "for audio_id, sub_df in submission.groupby(\"audio_id\"):\n",
    "    events = sub_df[[\"ebird_code\", \"onset\", \"offset\", \"max_confidence\"]].values\n",
    "    n_events = len(events)\n",
    "\n",
    "    for i in range(n_events):\n",
    "        event = events[i][0]\n",
    "        onset = events[i][1]\n",
    "        offset = events[i][2]\n",
    "\n",
    "        start_section = int((onset // 5) * 5) + 5\n",
    "        end_section = int((offset // 5) * 5) + 5\n",
    "        cur_section = start_section\n",
    "\n",
    "        row_id = f\"{audio_id}_{start_section}\"\n",
    "        if labels.get(row_id) is not None:\n",
    "            labels[row_id].add(event)\n",
    "        else:\n",
    "            labels[row_id] = set()\n",
    "            labels[row_id].add(event)\n",
    "\n",
    "        while cur_section != end_section:\n",
    "            cur_section += 5\n",
    "            row_id = f\"{audio_id}_{cur_section}\"\n",
    "            if labels.get(row_id) is not None:\n",
    "                labels[row_id].add(event)\n",
    "            else:\n",
    "                labels[row_id] = set()\n",
    "                labels[row_id].add(event)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "90f2c2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in labels:\n",
    "    labels[key] = \" \".join(sorted(list(labels[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "704989b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>birds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10534_SSW_45</td>\n",
       "      <td>dowwoo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10534_SSW_50</td>\n",
       "      <td>dowwoo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10534_SSW_165</td>\n",
       "      <td>dowwoo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10534_SSW_170</td>\n",
       "      <td>dowwoo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11254_COR_5</td>\n",
       "      <td>wbwwre1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          row_id    birds\n",
       "0   10534_SSW_45   dowwoo\n",
       "1   10534_SSW_50   dowwoo\n",
       "2  10534_SSW_165   dowwoo\n",
       "3  10534_SSW_170   dowwoo\n",
       "4    11254_COR_5  wbwwre1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_ids = list(labels.keys())\n",
    "birds = list(labels.values())\n",
    "post_processed = pd.DataFrame({\n",
    "    \"row_id\": row_ids,\n",
    "    \"birds\": birds\n",
    "})\n",
    "post_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7ce52216",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sub_raws = []\n",
    "for audio_path in submission_df['path']:\n",
    "#         with timer(f\"Loading {str(audio_path)}\", logger):\n",
    "    seconds = []\n",
    "    row_ids = []\n",
    "    for second in range(5, 605, 5):\n",
    "        row_id = \"_\".join(audio_path.name.split(\"_\")[:2]) + f\"_{second}\"\n",
    "        seconds.append(second)\n",
    "        row_ids.append(row_id)\n",
    "\n",
    "    test_df = pd.DataFrame({\n",
    "        \"row_id\": row_ids,\n",
    "        \"seconds\": seconds\n",
    "    })\n",
    "    sub_raws.append(test_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5657f58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_raws = pd.concat(sub_raws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "60afc95c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>birds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57610_COR_5</td>\n",
       "      <td>nocall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57610_COR_10</td>\n",
       "      <td>nocall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57610_COR_15</td>\n",
       "      <td>rubwre1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>57610_COR_20</td>\n",
       "      <td>nocall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57610_COR_25</td>\n",
       "      <td>nocall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>57610_COR_30</td>\n",
       "      <td>nocall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>57610_COR_35</td>\n",
       "      <td>nocall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>57610_COR_40</td>\n",
       "      <td>nocall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>57610_COR_45</td>\n",
       "      <td>nocall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>57610_COR_50</td>\n",
       "      <td>nocall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>57610_COR_55</td>\n",
       "      <td>nocall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>57610_COR_60</td>\n",
       "      <td>nocall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>57610_COR_65</td>\n",
       "      <td>rubwre1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>57610_COR_70</td>\n",
       "      <td>rubwre1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>57610_COR_75</td>\n",
       "      <td>nocall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>57610_COR_80</td>\n",
       "      <td>nocall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>57610_COR_85</td>\n",
       "      <td>nocall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>57610_COR_90</td>\n",
       "      <td>nocall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>57610_COR_95</td>\n",
       "      <td>nocall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>57610_COR_100</td>\n",
       "      <td>nocall</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           row_id    birds\n",
       "0     57610_COR_5   nocall\n",
       "1    57610_COR_10   nocall\n",
       "2    57610_COR_15  rubwre1\n",
       "3    57610_COR_20   nocall\n",
       "4    57610_COR_25   nocall\n",
       "5    57610_COR_30   nocall\n",
       "6    57610_COR_35   nocall\n",
       "7    57610_COR_40   nocall\n",
       "8    57610_COR_45   nocall\n",
       "9    57610_COR_50   nocall\n",
       "10   57610_COR_55   nocall\n",
       "11   57610_COR_60   nocall\n",
       "12   57610_COR_65  rubwre1\n",
       "13   57610_COR_70  rubwre1\n",
       "14   57610_COR_75   nocall\n",
       "15   57610_COR_80   nocall\n",
       "16   57610_COR_85   nocall\n",
       "17   57610_COR_90   nocall\n",
       "18   57610_COR_95   nocall\n",
       "19  57610_COR_100   nocall"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_row_id = sub_raws[[\"row_id\"]]\n",
    "submission = all_row_id.merge(post_processed, on=\"row_id\", how=\"left\")\n",
    "submission = submission.fillna(\"nocall\")\n",
    "submission.to_csv(OUTPUT_DIR / \"submission.csv\", index=False)\n",
    "submission.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ede35d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6346805555555493\n"
     ]
    }
   ],
   "source": [
    "if not SUBMISSION and not TEST:\n",
    "    sys.path.append('../../')\n",
    "    import src.row_wise_micro_averaged_f1 as mi_f1\n",
    "    \n",
    "    submission = pd.read_csv(OUTPUT_DIR / \"submission.csv\")\n",
    "    ans = pd.read_csv(MAIN_DATA_DIR / 'train_soundscape_labels.csv')\n",
    "    \n",
    "    submission = pd.merge(submission, ans, how='inner', on='row_id')\n",
    "    \n",
    "    f1_score = mi_f1.row_wise_micro_averaged_f1_score(list(submission['birds_y']), list(submission['birds_x']))\n",
    "    print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8715b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
