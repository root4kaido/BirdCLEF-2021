{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22447d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.6/site-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  '\"sox\" backend is being deprecated. '\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import timm\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as torchdata\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from timm.models.layers import SelectAdaptivePool2d\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torchlibrosa.stft import LogmelFilterBank, Spectrogram\n",
    "from torchlibrosa.augmentation import SpecAugmentation\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0526637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93695c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../')\n",
    "import src.utils as utils\n",
    "from config_ import CFG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98063174",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a15e7724",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"/home/knikaido/work/BirdCLEF2021/data/\")\n",
    "MAIN_DATA_DIR = DATA_DIR / 'birdclef-2021'\n",
    "OUTPUT_DIR = Path('./output/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b407dd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Data #\n",
    "######################\n",
    "train_datadir = MAIN_DATA_DIR / 'train_short_audio'\n",
    "train_csv = MAIN_DATA_DIR / 'train_metadata.csv'\n",
    "train_soundscape = MAIN_DATA_DIR / 'train_soundscape_labels.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "180c36e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "if DEBUG:\n",
    "    CFG.epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147a2bee",
   "metadata": {},
   "source": [
    "## My func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c8992e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveformDataset(torchdata.Dataset):\n",
    "    def __init__(self,\n",
    "                 df: pd.DataFrame,\n",
    "                 datadir: Path,\n",
    "                 img_size=224,\n",
    "                 waveform_transforms=None,\n",
    "                 period=20,\n",
    "                 validation=False):\n",
    "        self.df = df\n",
    "        self.datadir = datadir\n",
    "        self.img_size = img_size\n",
    "        self.waveform_transforms = waveform_transforms\n",
    "        self.period = period\n",
    "        self.validation = validation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        sample = self.df.loc[idx, :]\n",
    "        wav_name = sample[\"filename\"]\n",
    "        ebird_code = sample[\"primary_label\"]\n",
    "\n",
    "        y, sr = sf.read(self.datadir / ebird_code / wav_name)\n",
    "\n",
    "        len_y = len(y)\n",
    "        effective_length = sr * self.period\n",
    "        if len_y < effective_length:\n",
    "            new_y = np.zeros(effective_length, dtype=y.dtype)\n",
    "            if not self.validation:\n",
    "                start = np.random.randint(effective_length - len_y)\n",
    "            else:\n",
    "                start = 0\n",
    "            new_y[start:start + len_y] = y\n",
    "            y = new_y.astype(np.float32)\n",
    "        elif len_y > effective_length:\n",
    "            if not self.validation:\n",
    "                start = np.random.randint(len_y - effective_length)\n",
    "            else:\n",
    "                start = 0\n",
    "            y = y[start:start + effective_length].astype(np.float32)\n",
    "        else:\n",
    "            y = y.astype(np.float32)\n",
    "\n",
    "        y = np.nan_to_num(y)\n",
    "\n",
    "        if self.waveform_transforms:\n",
    "            y = self.waveform_transforms(y)\n",
    "\n",
    "        y = np.nan_to_num(y)\n",
    "\n",
    "        labels = np.zeros(len(CFG.target_columns), dtype=float) + 0.0025\n",
    "        labels[CFG.target_columns.index(ebird_code)] = 0.995\n",
    "\n",
    "        return {\n",
    "            \"image\": y,\n",
    "            \"targets\": labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52a78bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(phase: str):\n",
    "    transforms = CFG.transforms\n",
    "    if transforms is None:\n",
    "        return None\n",
    "    else:\n",
    "        if transforms[phase] is None:\n",
    "            return None\n",
    "        trns_list = []\n",
    "        for trns_conf in transforms[phase]:\n",
    "            trns_name = trns_conf[\"name\"]\n",
    "            trns_params = {} if trns_conf.get(\"params\") is None else \\\n",
    "                trns_conf[\"params\"]\n",
    "            if globals().get(trns_name) is not None:\n",
    "                trns_cls = globals()[trns_name]\n",
    "                trns_list.append(trns_cls(**trns_params))\n",
    "\n",
    "        if len(trns_list) > 0:\n",
    "            return Compose(trns_list)\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        \n",
    "class Normalize:\n",
    "    def __call__(self, y: np.ndarray):\n",
    "        max_vol = np.abs(y).max()\n",
    "        y_vol = y * 1 / max_vol\n",
    "        return np.asfortranarray(y_vol)\n",
    "\n",
    "\n",
    "class Compose:\n",
    "    def __init__(self, transforms: list):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, y: np.ndarray):\n",
    "        for trns in self.transforms:\n",
    "            y = trns(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb3d7ad",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f163933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layer(layer):\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "    if hasattr(layer, \"bias\"):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "\n",
    "\n",
    "def init_bn(bn):\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.0)\n",
    "\n",
    "\n",
    "def init_weights(model):\n",
    "    classname = model.__class__.__name__\n",
    "    if classname.find(\"Conv2d\") != -1:\n",
    "        nn.init.xavier_uniform_(model.weight, gain=np.sqrt(2))\n",
    "        model.bias.data.fill_(0)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        model.weight.data.normal_(1.0, 0.02)\n",
    "        model.bias.data.fill_(0)\n",
    "    elif classname.find(\"GRU\") != -1:\n",
    "        for weight in model.parameters():\n",
    "            if len(weight.size()) > 1:\n",
    "                nn.init.orghogonal_(weight.data)\n",
    "    elif classname.find(\"Linear\") != -1:\n",
    "        model.weight.data.normal_(0, 0.01)\n",
    "        model.bias.data.zero_()\n",
    "\n",
    "\n",
    "def do_mixup(x: torch.Tensor, mixup_lambda: torch.Tensor):\n",
    "    \"\"\"Mixup x of even indexes (0, 2, 4, ...) with x of odd indexes\n",
    "    (1, 3, 5, ...).\n",
    "    Args:\n",
    "      x: (batch_size * 2, ...)\n",
    "      mixup_lambda: (batch_size * 2,)\n",
    "    Returns:\n",
    "      out: (batch_size, ...)\n",
    "    \"\"\"\n",
    "    out = (x[0::2].transpose(0, -1) * mixup_lambda[0::2] +\n",
    "           x[1::2].transpose(0, -1) * mixup_lambda[1::2]).transpose(0, -1)\n",
    "    return out\n",
    "\n",
    "\n",
    "class Mixup(object):\n",
    "    def __init__(self, mixup_alpha, random_seed=1234):\n",
    "        \"\"\"Mixup coefficient generator.\n",
    "        \"\"\"\n",
    "        self.mixup_alpha = mixup_alpha\n",
    "        self.random_state = np.random.RandomState(random_seed)\n",
    "\n",
    "    def get_lambda(self, batch_size):\n",
    "        \"\"\"Get mixup random coefficients.\n",
    "        Args:\n",
    "          batch_size: int\n",
    "        Returns:\n",
    "          mixup_lambdas: (batch_size,)\n",
    "        \"\"\"\n",
    "        mixup_lambdas = []\n",
    "        for n in range(0, batch_size, 2):\n",
    "            lam = self.random_state.beta(\n",
    "                self.mixup_alpha, self.mixup_alpha, 1)[0]\n",
    "            mixup_lambdas.append(lam)\n",
    "            mixup_lambdas.append(1. - lam)\n",
    "\n",
    "        return torch.from_numpy(np.array(mixup_lambdas, dtype=np.float32))\n",
    "\n",
    "\n",
    "def interpolate(x: torch.Tensor, ratio: int):\n",
    "    \"\"\"Interpolate data in time domain. This is used to compensate the\n",
    "    resolution reduction in downsampling of a CNN.\n",
    "    Args:\n",
    "      x: (batch_size, time_steps, classes_num)\n",
    "      ratio: int, ratio to interpolate\n",
    "    Returns:\n",
    "      upsampled: (batch_size, time_steps * ratio, classes_num)\n",
    "    \"\"\"\n",
    "    (batch_size, time_steps, classes_num) = x.shape\n",
    "    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n",
    "    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n",
    "    return upsampled\n",
    "\n",
    "\n",
    "def pad_framewise_output(framewise_output: torch.Tensor, frames_num: int):\n",
    "    \"\"\"Pad framewise_output to the same length as input frames. The pad value\n",
    "    is the same as the value of the last frame.\n",
    "    Args:\n",
    "      framewise_output: (batch_size, frames_num, classes_num)\n",
    "      frames_num: int, number of frames to pad\n",
    "    Outputs:\n",
    "      output: (batch_size, frames_num, classes_num)\n",
    "    \"\"\"\n",
    "    output = F.interpolate(\n",
    "        framewise_output.unsqueeze(1),\n",
    "        size=(frames_num, framewise_output.size(2)),\n",
    "        align_corners=True,\n",
    "        mode=\"bilinear\").squeeze(1)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def gem(x: torch.Tensor, p=3, eps=1e-6):\n",
    "    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1. / p)\n",
    "\n",
    "\n",
    "class GeM(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.p = nn.Parameter(torch.ones(1) * p)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return gem(x, p=self.p, eps=self.eps)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + f\"(p={self.p.data.tolist()[0]:.4f}, eps={self.eps})\"\n",
    "\n",
    "\n",
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return x\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d1a15af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TimmSED(nn.Module):\n",
    "    def __init__(self, base_model_name: str, pretrained=False, num_classes=24, in_channels=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.melspectrogramer = torchaudio.transforms.MelSpectrogram(sample_rate=CFG.sample_rate, n_fft=CFG.n_fft, win_length=CFG.n_fft, \n",
    "                                                                     hop_length=CFG.hop_length, f_min=CFG.fmin, \n",
    "                                                                     f_max=CFG.fmax, n_mels=CFG.n_mels, power=1.0)\n",
    "        self.amplituder = torchaudio.transforms.AmplitudeToDB()\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(CFG.n_mels)\n",
    "\n",
    "        base_model = timm.create_model(\n",
    "            base_model_name, pretrained=pretrained, in_chans=in_channels)\n",
    "        layers = list(base_model.children())[:-2]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "\n",
    "        if hasattr(base_model, \"fc\"):\n",
    "            in_features = base_model.fc.in_features\n",
    "        else:\n",
    "            in_features = base_model.classifier.in_features\n",
    "        self.fc1 = nn.Linear(in_features, in_features, bias=True)\n",
    "        self.att_block = AttBlockV2(\n",
    "            in_features, num_classes, activation=\"sigmoid\")\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_layer(self.fc1)\n",
    "        init_bn(self.bn0)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # (batch_size, 1, mel_bins, time_steps)\n",
    "        \n",
    "        x = self.melspectrogramer(input)\n",
    "        x = self.amplituder(x).unsqueeze(1)\n",
    "        frames_num = x.shape[3]\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "#         if self.training:\n",
    "#             x = self.spec_augmenter(x)\n",
    "\n",
    "        # (batch_size, channels, freq, frames)\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # (batch_size, channels, frames)\n",
    "        x = torch.mean(x, dim=2)\n",
    "\n",
    "        # channel smoothing\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n",
    "        logit = torch.sum(norm_att * self.att_block.cla(x), dim=2)\n",
    "        segmentwise_logit = self.att_block.cla(x).transpose(1, 2)\n",
    "        segmentwise_output = segmentwise_output.transpose(1, 2)\n",
    "\n",
    "        interpolate_ratio = frames_num // segmentwise_output.size(1)\n",
    "\n",
    "        # Get framewise output\n",
    "        framewise_output = interpolate(segmentwise_output,\n",
    "                                       interpolate_ratio)\n",
    "        framewise_output = pad_framewise_output(framewise_output, frames_num)\n",
    "\n",
    "        framewise_logit = interpolate(segmentwise_logit, interpolate_ratio)\n",
    "        framewise_logit = pad_framewise_output(framewise_logit, frames_num)\n",
    "\n",
    "        output_dict = {\n",
    "            \"framewise_output\": framewise_output,\n",
    "            \"segmentwise_output\": segmentwise_output,\n",
    "            \"logit\": logit,\n",
    "            \"framewise_logit\": framewise_logit,\n",
    "            \"clipwise_output\": clipwise_output\n",
    "        }\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634752c7",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fddd1db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/rfcx-species-audio-detection/discussion/213075\n",
    "class BCEFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        bce_loss = nn.BCEWithLogitsLoss(reduction='none')(preds, targets)\n",
    "        probas = torch.sigmoid(preds)\n",
    "        loss = targets * self.alpha * \\\n",
    "            (1. - probas)**self.gamma * bce_loss + \\\n",
    "            (1. - targets) * probas**self.gamma * bce_loss\n",
    "        loss = loss.mean()\n",
    "        return loss\n",
    "\n",
    "\n",
    "class BCEFocal2WayLoss(nn.Module):\n",
    "    def __init__(self, weights=[1, 1], class_weights=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.focal = BCEFocalLoss()\n",
    "\n",
    "        self.weights = weights\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        input_ = input[\"logit\"]\n",
    "        target = target.float()\n",
    "\n",
    "        framewise_output = input[\"framewise_logit\"]\n",
    "        clipwise_output_with_max, _ = framewise_output.max(dim=1)\n",
    "\n",
    "        loss = self.focal(input_, target)\n",
    "        aux_loss = self.focal(clipwise_output_with_max, target)\n",
    "\n",
    "        return self.weights[0] * loss + self.weights[1] * aux_loss\n",
    "    \n",
    "class BCEFocalClipLoss(nn.Module):\n",
    "    def __init__(self, weights=[1, 1], class_weights=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.focal = BCEFocalLoss()\n",
    "\n",
    "        self.weights = weights\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        input_ = input[\"logit\"]\n",
    "        target = target.float()\n",
    "\n",
    "        clipwise_output = input[\"clipwise_output\"]\n",
    "\n",
    "        loss = self.focal(input_, target)\n",
    "        aux_loss = self.focal(clipwise_output, target)\n",
    "\n",
    "        return self.weights[0] * loss + self.weights[1] * aux_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9970bffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "__CRITERIONS__ = {\n",
    "    \"BCEFocalLoss\": BCEFocalLoss,\n",
    "    \"BCEFocal2WayLoss\": BCEFocal2WayLoss,\n",
    "    \"BCEFocalClipLoss\": BCEFocalClipLoss\n",
    "}\n",
    "\n",
    "\n",
    "def get_criterion():\n",
    "    if hasattr(nn, CFG.loss_name):\n",
    "        return nn.__getattribute__(CFG.loss_name)(**CFG.loss_params)\n",
    "    elif __CRITERIONS__.get(CFG.loss_name) is not None:\n",
    "        return __CRITERIONS__[CFG.loss_name](**CFG.loss_params)\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9b006f",
   "metadata": {},
   "source": [
    "## Training Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0e9c8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom optimizer\n",
    "__OPTIMIZERS__ = {}\n",
    "\n",
    "\n",
    "def get_optimizer(model: nn.Module):\n",
    "    optimizer_name = CFG.optimizer_name\n",
    "    if optimizer_name == \"SAM\":\n",
    "        base_optimizer_name = CFG.base_optimizer\n",
    "        if __OPTIMIZERS__.get(base_optimizer_name) is not None:\n",
    "            base_optimizer = __OPTIMIZERS__[base_optimizer_name]\n",
    "        else:\n",
    "            base_optimizer = optim.__getattribute__(base_optimizer_name)\n",
    "        return SAM(model.parameters(), base_optimizer, **CFG.optimizer_params)\n",
    "\n",
    "    if __OPTIMIZERS__.get(optimizer_name) is not None:\n",
    "        return __OPTIMIZERS__[optimizer_name](model.parameters(),\n",
    "                                              **CFG.optimizer_params)\n",
    "    else:\n",
    "        return optim.__getattribute__(optimizer_name)(model.parameters(),\n",
    "                                                      **CFG.optimizer_params)\n",
    "\n",
    "\n",
    "def get_scheduler(optimizer):\n",
    "    scheduler_name = CFG.scheduler_name\n",
    "\n",
    "    if scheduler_name is None:\n",
    "        return\n",
    "    else:\n",
    "        return optim.lr_scheduler.__getattribute__(scheduler_name)(\n",
    "            optimizer, **CFG.scheduler_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b3f419",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a0c61b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "utils.set_seed(CFG.seed)\n",
    "\n",
    "# validation\n",
    "splitter = getattr(model_selection, CFG.split)(**CFG.split_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ecd2c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>primary_label</th>\n",
       "      <th>secondary_labels</th>\n",
       "      <th>type</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>scientific_name</th>\n",
       "      <th>common_name</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>filename</th>\n",
       "      <th>license</th>\n",
       "      <th>rating</th>\n",
       "      <th>time</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acafly</td>\n",
       "      <td>['amegfi']</td>\n",
       "      <td>['begging call', 'call', 'juvenile']</td>\n",
       "      <td>35.3860</td>\n",
       "      <td>-84.1250</td>\n",
       "      <td>Empidonax virescens</td>\n",
       "      <td>Acadian Flycatcher</td>\n",
       "      <td>Mike Nelson</td>\n",
       "      <td>2012-08-12</td>\n",
       "      <td>XC109605.ogg</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>09:30</td>\n",
       "      <td>https://www.xeno-canto.org/109605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acafly</td>\n",
       "      <td>[]</td>\n",
       "      <td>['call']</td>\n",
       "      <td>9.1334</td>\n",
       "      <td>-79.6501</td>\n",
       "      <td>Empidonax virescens</td>\n",
       "      <td>Acadian Flycatcher</td>\n",
       "      <td>Allen T. Chartier</td>\n",
       "      <td>2000-12-26</td>\n",
       "      <td>XC11209.ogg</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>?</td>\n",
       "      <td>https://www.xeno-canto.org/11209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acafly</td>\n",
       "      <td>[]</td>\n",
       "      <td>['call']</td>\n",
       "      <td>5.7813</td>\n",
       "      <td>-75.7452</td>\n",
       "      <td>Empidonax virescens</td>\n",
       "      <td>Acadian Flycatcher</td>\n",
       "      <td>Sergio Chaparro-Herrera</td>\n",
       "      <td>2012-01-10</td>\n",
       "      <td>XC127032.ogg</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15:20</td>\n",
       "      <td>https://www.xeno-canto.org/127032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acafly</td>\n",
       "      <td>['whwbec1']</td>\n",
       "      <td>['call']</td>\n",
       "      <td>4.6717</td>\n",
       "      <td>-75.6283</td>\n",
       "      <td>Empidonax virescens</td>\n",
       "      <td>Acadian Flycatcher</td>\n",
       "      <td>Oscar Humberto Marin-Gomez</td>\n",
       "      <td>2009-06-19</td>\n",
       "      <td>XC129974.ogg</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>07:50</td>\n",
       "      <td>https://www.xeno-canto.org/129974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acafly</td>\n",
       "      <td>['whwbec1']</td>\n",
       "      <td>['call']</td>\n",
       "      <td>4.6717</td>\n",
       "      <td>-75.6283</td>\n",
       "      <td>Empidonax virescens</td>\n",
       "      <td>Acadian Flycatcher</td>\n",
       "      <td>Oscar Humberto Marin-Gomez</td>\n",
       "      <td>2009-06-19</td>\n",
       "      <td>XC129981.ogg</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>07:50</td>\n",
       "      <td>https://www.xeno-canto.org/129981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62869</th>\n",
       "      <td>yetvir</td>\n",
       "      <td>[]</td>\n",
       "      <td>['adult', 'male', 'song']</td>\n",
       "      <td>30.2150</td>\n",
       "      <td>-97.6505</td>\n",
       "      <td>Vireo flavifrons</td>\n",
       "      <td>Yellow-throated Vireo</td>\n",
       "      <td>Caleb Helsel</td>\n",
       "      <td>2020-07-10</td>\n",
       "      <td>XC591680.ogg</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>08:30</td>\n",
       "      <td>https://www.xeno-canto.org/591680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62870</th>\n",
       "      <td>yetvir</td>\n",
       "      <td>[]</td>\n",
       "      <td>['life stage uncertain', 'sex uncertain', 'song']</td>\n",
       "      <td>42.3005</td>\n",
       "      <td>-72.5877</td>\n",
       "      <td>Vireo flavifrons</td>\n",
       "      <td>Yellow-throated Vireo</td>\n",
       "      <td>Christopher McPherson</td>\n",
       "      <td>2019-05-31</td>\n",
       "      <td>XC600085.ogg</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>09:30</td>\n",
       "      <td>https://www.xeno-canto.org/600085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62871</th>\n",
       "      <td>yetvir</td>\n",
       "      <td>['amered', 'eawpew', 'norcar', 'reevir1']</td>\n",
       "      <td>['adult', 'male', 'song']</td>\n",
       "      <td>42.3005</td>\n",
       "      <td>-72.5877</td>\n",
       "      <td>Vireo flavifrons</td>\n",
       "      <td>Yellow-throated Vireo</td>\n",
       "      <td>Christopher McPherson</td>\n",
       "      <td>2020-06-02</td>\n",
       "      <td>XC602701.ogg</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>08:30</td>\n",
       "      <td>https://www.xeno-canto.org/602701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62872</th>\n",
       "      <td>yetvir</td>\n",
       "      <td>[]</td>\n",
       "      <td>['uncertain']</td>\n",
       "      <td>32.2357</td>\n",
       "      <td>-99.8811</td>\n",
       "      <td>Vireo flavifrons</td>\n",
       "      <td>Yellow-throated Vireo</td>\n",
       "      <td>Brad Banner</td>\n",
       "      <td>2019-04-27</td>\n",
       "      <td>XC614733.ogg</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17:30</td>\n",
       "      <td>https://www.xeno-canto.org/614733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62873</th>\n",
       "      <td>yetvir</td>\n",
       "      <td>['gamqua', 'whwdov']</td>\n",
       "      <td>['adult', 'male', 'song']</td>\n",
       "      <td>31.9060</td>\n",
       "      <td>-109.1543</td>\n",
       "      <td>Vireo flavifrons</td>\n",
       "      <td>Yellow-throated Vireo</td>\n",
       "      <td>Richard E. Webster</td>\n",
       "      <td>2020-05-26</td>\n",
       "      <td>XC615888.ogg</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>06:23</td>\n",
       "      <td>https://www.xeno-canto.org/615888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62874 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      primary_label                           secondary_labels  \\\n",
       "0            acafly                                 ['amegfi']   \n",
       "1            acafly                                         []   \n",
       "2            acafly                                         []   \n",
       "3            acafly                                ['whwbec1']   \n",
       "4            acafly                                ['whwbec1']   \n",
       "...             ...                                        ...   \n",
       "62869        yetvir                                         []   \n",
       "62870        yetvir                                         []   \n",
       "62871        yetvir  ['amered', 'eawpew', 'norcar', 'reevir1']   \n",
       "62872        yetvir                                         []   \n",
       "62873        yetvir                       ['gamqua', 'whwdov']   \n",
       "\n",
       "                                                    type  latitude  longitude  \\\n",
       "0                   ['begging call', 'call', 'juvenile']   35.3860   -84.1250   \n",
       "1                                               ['call']    9.1334   -79.6501   \n",
       "2                                               ['call']    5.7813   -75.7452   \n",
       "3                                               ['call']    4.6717   -75.6283   \n",
       "4                                               ['call']    4.6717   -75.6283   \n",
       "...                                                  ...       ...        ...   \n",
       "62869                          ['adult', 'male', 'song']   30.2150   -97.6505   \n",
       "62870  ['life stage uncertain', 'sex uncertain', 'song']   42.3005   -72.5877   \n",
       "62871                          ['adult', 'male', 'song']   42.3005   -72.5877   \n",
       "62872                                      ['uncertain']   32.2357   -99.8811   \n",
       "62873                          ['adult', 'male', 'song']   31.9060  -109.1543   \n",
       "\n",
       "           scientific_name            common_name                      author  \\\n",
       "0      Empidonax virescens     Acadian Flycatcher                 Mike Nelson   \n",
       "1      Empidonax virescens     Acadian Flycatcher           Allen T. Chartier   \n",
       "2      Empidonax virescens     Acadian Flycatcher     Sergio Chaparro-Herrera   \n",
       "3      Empidonax virescens     Acadian Flycatcher  Oscar Humberto Marin-Gomez   \n",
       "4      Empidonax virescens     Acadian Flycatcher  Oscar Humberto Marin-Gomez   \n",
       "...                    ...                    ...                         ...   \n",
       "62869     Vireo flavifrons  Yellow-throated Vireo                Caleb Helsel   \n",
       "62870     Vireo flavifrons  Yellow-throated Vireo       Christopher McPherson   \n",
       "62871     Vireo flavifrons  Yellow-throated Vireo       Christopher McPherson   \n",
       "62872     Vireo flavifrons  Yellow-throated Vireo                 Brad Banner   \n",
       "62873     Vireo flavifrons  Yellow-throated Vireo          Richard E. Webster   \n",
       "\n",
       "             date      filename  \\\n",
       "0      2012-08-12  XC109605.ogg   \n",
       "1      2000-12-26   XC11209.ogg   \n",
       "2      2012-01-10  XC127032.ogg   \n",
       "3      2009-06-19  XC129974.ogg   \n",
       "4      2009-06-19  XC129981.ogg   \n",
       "...           ...           ...   \n",
       "62869  2020-07-10  XC591680.ogg   \n",
       "62870  2019-05-31  XC600085.ogg   \n",
       "62871  2020-06-02  XC602701.ogg   \n",
       "62872  2019-04-27  XC614733.ogg   \n",
       "62873  2020-05-26  XC615888.ogg   \n",
       "\n",
       "                                                 license  rating   time  \\\n",
       "0      Creative Commons Attribution-NonCommercial-Sha...     2.5  09:30   \n",
       "1      Creative Commons Attribution-NonCommercial-Sha...     3.0      ?   \n",
       "2      Creative Commons Attribution-NonCommercial-Sha...     3.0  15:20   \n",
       "3      Creative Commons Attribution-NonCommercial-Sha...     3.5  07:50   \n",
       "4      Creative Commons Attribution-NonCommercial-Sha...     3.5  07:50   \n",
       "...                                                  ...     ...    ...   \n",
       "62869  Creative Commons Attribution-NonCommercial-Sha...     1.0  08:30   \n",
       "62870  Creative Commons Attribution-NonCommercial-Sha...     5.0  09:30   \n",
       "62871  Creative Commons Attribution-NonCommercial-Sha...     4.5  08:30   \n",
       "62872  Creative Commons Attribution-NonCommercial-Sha...     4.0  17:30   \n",
       "62873  Creative Commons Attribution-NonCommercial-Sha...     4.5  06:23   \n",
       "\n",
       "                                     url  \n",
       "0      https://www.xeno-canto.org/109605  \n",
       "1       https://www.xeno-canto.org/11209  \n",
       "2      https://www.xeno-canto.org/127032  \n",
       "3      https://www.xeno-canto.org/129974  \n",
       "4      https://www.xeno-canto.org/129981  \n",
       "...                                  ...  \n",
       "62869  https://www.xeno-canto.org/591680  \n",
       "62870  https://www.xeno-canto.org/600085  \n",
       "62871  https://www.xeno-canto.org/602701  \n",
       "62872  https://www.xeno-canto.org/614733  \n",
       "62873  https://www.xeno-canto.org/615888  \n",
       "\n",
       "[62874 rows x 14 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(MAIN_DATA_DIR / 'train_metadata.csv')\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00af387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learner class(pytorch-lighting)\n",
    "class Learner(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = get_criterion()\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        b_data = batch\n",
    "        output = self.model(b_data['image'])\n",
    "        loss = self.criterion(output, b_data[\"targets\"])\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        b_data = batch\n",
    "        output = self.model(b_data['image'])\n",
    "        loss = self.criterion(output, b_data[\"targets\"])\n",
    "        \n",
    "        # floor lossは現状は無視して良い\n",
    "        self.log(f'Loss/val', loss, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x for x in outputs]).mean()\n",
    "        print(f'epoch = {self.current_epoch}, loss = {avg_loss}')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = get_optimizer(self.model)\n",
    "        scheduler = get_scheduler(optimizer)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"Loss/val\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fe3063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msqrt4kaido\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">smart-surf-47</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/sqrt4kaido/BirdCLEF-2021\" target=\"_blank\">https://wandb.ai/sqrt4kaido/BirdCLEF-2021</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/sqrt4kaido/BirdCLEF-2021/runs/2xffg003\" target=\"_blank\">https://wandb.ai/sqrt4kaido/BirdCLEF-2021/runs/2xffg003</a><br/>\n",
       "                Run data is saved locally in <code>/home/knikaido/work/BirdCLEF2021/Git/Notebook/16/wandb/run-20210519_011637-2xffg003</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | TimmSED          | 8.5 M \n",
      "1 | criterion | BCEFocalClipLoss | 0     \n",
      "-----------------------------------------------\n",
      "8.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.5 M     Total params\n",
      "34.061    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/user/.local/lib/python3.6/site-packages/torch/functional.py:516: UserWarning: stft will require the return_complex parameter be explicitly  specified in a future PyTorch release. Use return_complex=False  to preserve the current behavior or return_complex=True to return  a complex output. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:653.)\n",
      "  normalized, onesided, return_complex)\n",
      "/home/user/.local/lib/python3.6/site-packages/torch/functional.py:516: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:590.)\n",
      "  normalized, onesided, return_complex)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0, loss = 0.9247585535049438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f058cbb962f48c2aaca28ceb80fa490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0, loss = 0.17584900557994843\n"
     ]
    }
   ],
   "source": [
    "for i, (trn_idx, val_idx) in enumerate(splitter.split(train, y=train[\"primary_label\"])):\n",
    "    if i not in CFG.folds:\n",
    "        continue\n",
    "\n",
    "    trn_df = train.loc[trn_idx, :].reset_index(drop=True)\n",
    "    val_df = train.loc[val_idx, :].reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    loaders = {\n",
    "        phase: torchdata.DataLoader(\n",
    "            WaveformDataset(\n",
    "                df_,\n",
    "                train_datadir,\n",
    "                img_size=CFG.img_size,\n",
    "                waveform_transforms=get_transforms(phase),\n",
    "                period=CFG.period,\n",
    "                validation=(phase == \"valid\")\n",
    "            ),\n",
    "            **CFG.loader_params[phase])  # type: ignore\n",
    "        for phase, df_ in zip([\"train\", \"valid\"], [trn_df, val_df])\n",
    "    }\n",
    "    \n",
    "    model = TimmSED(\n",
    "        base_model_name=CFG.base_model_name,\n",
    "        pretrained=CFG.pretrained,\n",
    "        num_classes=CFG.num_classes,\n",
    "        in_channels=CFG.in_channels)\n",
    "    model_name = model.__class__.__name__\n",
    "    \n",
    "    learner = Learner(model)\n",
    "    \n",
    "    # loggers\n",
    "    RUN_NAME = f'exp{str(CFG.exp_num)}'\n",
    "    wandb.init(project='BirdCLEF-2021', entity='sqrt4kaido', group=RUN_NAME, job_type=RUN_NAME + f'-fold-{i}')\n",
    "    wandb.run.name = RUN_NAME + f'-fold-{i}'\n",
    "    wandb_config = wandb.config\n",
    "    wandb_config.model_name = model_name\n",
    "    wandb.watch(model)\n",
    "    \n",
    "    # callbacks\n",
    "    callbacks = []\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=f'Loss/val',\n",
    "        mode='min',\n",
    "        dirpath=OUTPUT_DIR,\n",
    "        verbose=False,\n",
    "        filename=f'{model_name}-{learner.current_epoch}-{i}')\n",
    "    callbacks.append(checkpoint_callback)\n",
    "\n",
    "#     early_stop_callback = EarlyStopping(\n",
    "#         monitor='Loss/val',\n",
    "#         min_delta=0.00,\n",
    "#         patience=20,\n",
    "#         verbose=True,\n",
    "#         mode='min')\n",
    "#     callbacks.append(early_stop_callback)\n",
    "    \n",
    "    loggers = []\n",
    "    loggers.append(WandbLogger())\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        logger=loggers,\n",
    "        callbacks=callbacks,\n",
    "        max_epochs=CFG.epochs,\n",
    "        default_root_dir=OUTPUT_DIR,\n",
    "        gpus=1,\n",
    "        fast_dev_run=DEBUG,\n",
    "        deterministic=True,\n",
    "        benchmark=True,\n",
    "        )\n",
    "    \n",
    "    trainer.fit(learner, train_dataloader=loaders['train'], val_dataloaders=loaders['valid'])\n",
    "    \n",
    "    trainer.save_checkpoint(OUTPUT_DIR / \"last.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7613a542",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project='BirdCLEF-2021', entity='sqrt4kaido', group=RUN_NAME, job_type='summary')\n",
    "wandb.run.name = 'summary'\n",
    "# wandb.log({'CV_score': oofs_score})\n",
    "wandb.save('./config_.py')\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426d64cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
